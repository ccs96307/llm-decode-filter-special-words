{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"./models/google--gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.bfloat16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_words = [\"talk\", \"listen\", \"fuck you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered First Candidates: [['▁t'], ['▁tal'], ['fuck'], ['▁f'], ['tal'], ['f'], ['ta'], ['▁fuck'], ['▁list'], ['li'], ['▁fu'], ['▁li'], ['▁ta'], ['l'], ['listen'], ['list'], ['▁liste'], ['▁l'], ['▁listen'], ['fu'], ['▁talk'], ['lis'], ['liste'], ['t'], ['▁fuc'], ['talk'], ['▁lis']]\n",
      "Filtered First IDs: [[474], [3412], [34024], [517], [3559], [235266], [516], [7935], [1889], [515], [4936], [702], [3586], [235257], [18998], [1701], [32165], [533], [10724], [12819], [5063], [15063], [44003], [235251], [79433], [33085], [23966]]\n",
      "(beginning) Final Routes: [['listen'], ['▁listen'], ['▁talk'], ['talk']]\n",
      "(beginning) Final ID Routes: [[18998], [10724], [5063], [33085]]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you']]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u']]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u'], ['▁t', 'a', 'l', 'k'], ['t', 'a', 'l', 'k'], ['l', 'i', 's', 'ten'], ['▁l', 'i', 's', 'ten'], ['▁fu', 'ck', '▁y', 'ou'], ['fu', 'ck', '▁y', 'ou'], ['▁fuc', 'k', '▁y', 'ou'], ['▁f', 'uck', '▁y', 'ou'], ['f', 'uck', '▁y', 'ou'], ['l', 'i', 'ste', 'n'], ['▁l', 'i', 'ste', 'n'], ['l', 'is', 'te', 'n'], ['▁l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['▁li', 's', 'te', 'n'], ['l', 'ist', 'e', 'n'], ['▁l', 'ist', 'e', 'n'], ['li', 'st', 'e', 'n'], ['▁li', 'st', 'e', 'n'], ['lis', 't', 'e', 'n'], ['▁lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['▁l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['▁l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['▁li', 's', 't', 'en'], ['▁f', 'u', 'ck', '▁you'], ['f', 'u', 'ck', '▁you'], ['▁fu', 'c', 'k', '▁you'], ['fu', 'c', 'k', '▁you'], ['▁f', 'uc', 'k', '▁you'], ['f', 'uc', 'k', '▁you'], ['fuck', '▁y', 'o', 'u'], ['▁fuck', '▁y', 'o', 'u'], ['▁fu', 'ck', '▁yo', 'u'], ['fu', 'ck', '▁yo', 'u'], ['▁fuc', 'k', '▁yo', 'u'], ['▁f', 'uck', '▁yo', 'u'], ['f', 'uck', '▁yo', 'u']]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u'], ['▁t', 'a', 'l', 'k'], ['t', 'a', 'l', 'k'], ['l', 'i', 's', 'ten'], ['▁l', 'i', 's', 'ten'], ['▁fu', 'ck', '▁y', 'ou'], ['fu', 'ck', '▁y', 'ou'], ['▁fuc', 'k', '▁y', 'ou'], ['▁f', 'uck', '▁y', 'ou'], ['f', 'uck', '▁y', 'ou'], ['l', 'i', 'ste', 'n'], ['▁l', 'i', 'ste', 'n'], ['l', 'is', 'te', 'n'], ['▁l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['▁li', 's', 'te', 'n'], ['l', 'ist', 'e', 'n'], ['▁l', 'ist', 'e', 'n'], ['li', 'st', 'e', 'n'], ['▁li', 'st', 'e', 'n'], ['lis', 't', 'e', 'n'], ['▁lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['▁l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['▁l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['▁li', 's', 't', 'en'], ['▁f', 'u', 'ck', '▁you'], ['f', 'u', 'ck', '▁you'], ['▁fu', 'c', 'k', '▁you'], ['fu', 'c', 'k', '▁you'], ['▁f', 'uc', 'k', '▁you'], ['f', 'uc', 'k', '▁you'], ['fuck', '▁y', 'o', 'u'], ['▁fuck', '▁y', 'o', 'u'], ['▁fu', 'ck', '▁yo', 'u'], ['fu', 'ck', '▁yo', 'u'], ['▁fuc', 'k', '▁yo', 'u'], ['▁f', 'uck', '▁yo', 'u'], ['f', 'uck', '▁yo', 'u'], ['▁f', 'u', 'ck', '▁y', 'ou'], ['f', 'u', 'ck', '▁y', 'ou'], ['▁fu', 'c', 'k', '▁y', 'ou'], ['fu', 'c', 'k', '▁y', 'ou'], ['▁f', 'uc', 'k', '▁y', 'ou'], ['f', 'uc', 'k', '▁y', 'ou'], ['l', 'i', 's', 'te', 'n'], ['▁l', 'i', 's', 'te', 'n'], ['l', 'i', 'st', 'e', 'n'], ['▁l', 'i', 'st', 'e', 'n'], ['l', 'is', 't', 'e', 'n'], ['▁l', 'is', 't', 'e', 'n'], ['li', 's', 't', 'e', 'n'], ['▁li', 's', 't', 'e', 'n'], ['l', 'i', 's', 't', 'en'], ['▁l', 'i', 's', 't', 'en'], ['▁f', 'u', 'c', 'k', '▁you'], ['f', 'u', 'c', 'k', '▁you'], ['▁fu', 'ck', '▁y', 'o', 'u'], ['fu', 'ck', '▁y', 'o', 'u'], ['▁fuc', 'k', '▁y', 'o', 'u'], ['▁f', 'uck', '▁y', 'o', 'u'], ['f', 'uck', '▁y', 'o', 'u'], ['▁f', 'u', 'ck', '▁yo', 'u'], ['f', 'u', 'ck', '▁yo', 'u'], ['▁fu', 'c', 'k', '▁yo', 'u'], ['fu', 'c', 'k', '▁yo', 'u'], ['▁f', 'uc', 'k', '▁yo', 'u'], ['f', 'uc', 'k', '▁yo', 'u']]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u'], ['▁t', 'a', 'l', 'k'], ['t', 'a', 'l', 'k'], ['l', 'i', 's', 'ten'], ['▁l', 'i', 's', 'ten'], ['▁fu', 'ck', '▁y', 'ou'], ['fu', 'ck', '▁y', 'ou'], ['▁fuc', 'k', '▁y', 'ou'], ['▁f', 'uck', '▁y', 'ou'], ['f', 'uck', '▁y', 'ou'], ['l', 'i', 'ste', 'n'], ['▁l', 'i', 'ste', 'n'], ['l', 'is', 'te', 'n'], ['▁l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['▁li', 's', 'te', 'n'], ['l', 'ist', 'e', 'n'], ['▁l', 'ist', 'e', 'n'], ['li', 'st', 'e', 'n'], ['▁li', 'st', 'e', 'n'], ['lis', 't', 'e', 'n'], ['▁lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['▁l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['▁l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['▁li', 's', 't', 'en'], ['▁f', 'u', 'ck', '▁you'], ['f', 'u', 'ck', '▁you'], ['▁fu', 'c', 'k', '▁you'], ['fu', 'c', 'k', '▁you'], ['▁f', 'uc', 'k', '▁you'], ['f', 'uc', 'k', '▁you'], ['fuck', '▁y', 'o', 'u'], ['▁fuck', '▁y', 'o', 'u'], ['▁fu', 'ck', '▁yo', 'u'], ['fu', 'ck', '▁yo', 'u'], ['▁fuc', 'k', '▁yo', 'u'], ['▁f', 'uck', '▁yo', 'u'], ['f', 'uck', '▁yo', 'u'], ['▁f', 'u', 'ck', '▁y', 'ou'], ['f', 'u', 'ck', '▁y', 'ou'], ['▁fu', 'c', 'k', '▁y', 'ou'], ['fu', 'c', 'k', '▁y', 'ou'], ['▁f', 'uc', 'k', '▁y', 'ou'], ['f', 'uc', 'k', '▁y', 'ou'], ['l', 'i', 's', 'te', 'n'], ['▁l', 'i', 's', 'te', 'n'], ['l', 'i', 'st', 'e', 'n'], ['▁l', 'i', 'st', 'e', 'n'], ['l', 'is', 't', 'e', 'n'], ['▁l', 'is', 't', 'e', 'n'], ['li', 's', 't', 'e', 'n'], ['▁li', 's', 't', 'e', 'n'], ['l', 'i', 's', 't', 'en'], ['▁l', 'i', 's', 't', 'en'], ['▁f', 'u', 'c', 'k', '▁you'], ['f', 'u', 'c', 'k', '▁you'], ['▁fu', 'ck', '▁y', 'o', 'u'], ['fu', 'ck', '▁y', 'o', 'u'], ['▁fuc', 'k', '▁y', 'o', 'u'], ['▁f', 'uck', '▁y', 'o', 'u'], ['f', 'uck', '▁y', 'o', 'u'], ['▁f', 'u', 'ck', '▁yo', 'u'], ['f', 'u', 'ck', '▁yo', 'u'], ['▁fu', 'c', 'k', '▁yo', 'u'], ['fu', 'c', 'k', '▁yo', 'u'], ['▁f', 'uc', 'k', '▁yo', 'u'], ['f', 'uc', 'k', '▁yo', 'u'], ['▁f', 'u', 'c', 'k', '▁y', 'ou'], ['f', 'u', 'c', 'k', '▁y', 'ou'], ['l', 'i', 's', 't', 'e', 'n'], ['▁l', 'i', 's', 't', 'e', 'n'], ['▁f', 'u', 'ck', '▁y', 'o', 'u'], ['f', 'u', 'ck', '▁y', 'o', 'u'], ['▁fu', 'c', 'k', '▁y', 'o', 'u'], ['fu', 'c', 'k', '▁y', 'o', 'u'], ['▁f', 'uc', 'k', '▁y', 'o', 'u'], ['f', 'uc', 'k', '▁y', 'o', 'u'], ['▁f', 'u', 'c', 'k', '▁yo', 'u'], ['f', 'u', 'c', 'k', '▁yo', 'u']]\n",
      "[['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u'], ['▁t', 'a', 'l', 'k'], ['t', 'a', 'l', 'k'], ['l', 'i', 's', 'ten'], ['▁l', 'i', 's', 'ten'], ['▁fu', 'ck', '▁y', 'ou'], ['fu', 'ck', '▁y', 'ou'], ['▁fuc', 'k', '▁y', 'ou'], ['▁f', 'uck', '▁y', 'ou'], ['f', 'uck', '▁y', 'ou'], ['l', 'i', 'ste', 'n'], ['▁l', 'i', 'ste', 'n'], ['l', 'is', 'te', 'n'], ['▁l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['▁li', 's', 'te', 'n'], ['l', 'ist', 'e', 'n'], ['▁l', 'ist', 'e', 'n'], ['li', 'st', 'e', 'n'], ['▁li', 'st', 'e', 'n'], ['lis', 't', 'e', 'n'], ['▁lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['▁l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['▁l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['▁li', 's', 't', 'en'], ['▁f', 'u', 'ck', '▁you'], ['f', 'u', 'ck', '▁you'], ['▁fu', 'c', 'k', '▁you'], ['fu', 'c', 'k', '▁you'], ['▁f', 'uc', 'k', '▁you'], ['f', 'uc', 'k', '▁you'], ['fuck', '▁y', 'o', 'u'], ['▁fuck', '▁y', 'o', 'u'], ['▁fu', 'ck', '▁yo', 'u'], ['fu', 'ck', '▁yo', 'u'], ['▁fuc', 'k', '▁yo', 'u'], ['▁f', 'uck', '▁yo', 'u'], ['f', 'uck', '▁yo', 'u'], ['▁f', 'u', 'ck', '▁y', 'ou'], ['f', 'u', 'ck', '▁y', 'ou'], ['▁fu', 'c', 'k', '▁y', 'ou'], ['fu', 'c', 'k', '▁y', 'ou'], ['▁f', 'uc', 'k', '▁y', 'ou'], ['f', 'uc', 'k', '▁y', 'ou'], ['l', 'i', 's', 'te', 'n'], ['▁l', 'i', 's', 'te', 'n'], ['l', 'i', 'st', 'e', 'n'], ['▁l', 'i', 'st', 'e', 'n'], ['l', 'is', 't', 'e', 'n'], ['▁l', 'is', 't', 'e', 'n'], ['li', 's', 't', 'e', 'n'], ['▁li', 's', 't', 'e', 'n'], ['l', 'i', 's', 't', 'en'], ['▁l', 'i', 's', 't', 'en'], ['▁f', 'u', 'c', 'k', '▁you'], ['f', 'u', 'c', 'k', '▁you'], ['▁fu', 'ck', '▁y', 'o', 'u'], ['fu', 'ck', '▁y', 'o', 'u'], ['▁fuc', 'k', '▁y', 'o', 'u'], ['▁f', 'uck', '▁y', 'o', 'u'], ['f', 'uck', '▁y', 'o', 'u'], ['▁f', 'u', 'ck', '▁yo', 'u'], ['f', 'u', 'ck', '▁yo', 'u'], ['▁fu', 'c', 'k', '▁yo', 'u'], ['fu', 'c', 'k', '▁yo', 'u'], ['▁f', 'uc', 'k', '▁yo', 'u'], ['f', 'uc', 'k', '▁yo', 'u'], ['▁f', 'u', 'c', 'k', '▁y', 'ou'], ['f', 'u', 'c', 'k', '▁y', 'ou'], ['l', 'i', 's', 't', 'e', 'n'], ['▁l', 'i', 's', 't', 'e', 'n'], ['▁f', 'u', 'ck', '▁y', 'o', 'u'], ['f', 'u', 'ck', '▁y', 'o', 'u'], ['▁fu', 'c', 'k', '▁y', 'o', 'u'], ['fu', 'c', 'k', '▁y', 'o', 'u'], ['▁f', 'uc', 'k', '▁y', 'o', 'u'], ['f', 'uc', 'k', '▁y', 'o', 'u'], ['▁f', 'u', 'c', 'k', '▁yo', 'u'], ['f', 'u', 'c', 'k', '▁yo', 'u'], ['▁f', 'u', 'c', 'k', '▁y', 'o', 'u'], ['f', 'u', 'c', 'k', '▁y', 'o', 'u']]\n",
      "Final Routes: [['listen'], ['▁listen'], ['▁talk'], ['talk'], ['▁tal', 'k'], ['tal', 'k'], ['li', 'sten'], ['▁li', 'sten'], ['ta', 'lk'], ['▁ta', 'lk'], ['lis', 'ten'], ['▁lis', 'ten'], ['▁t', 'alk'], ['t', 'alk'], ['▁liste', 'n'], ['liste', 'n'], ['▁list', 'en'], ['list', 'en'], ['l', 'isten'], ['▁l', 'isten'], ['fuck', '▁you'], ['▁fuck', '▁you'], ['ta', 'l', 'k'], ['▁ta', 'l', 'k'], ['▁t', 'al', 'k'], ['t', 'al', 'k'], ['l', 'i', 'sten'], ['▁l', 'i', 'sten'], ['▁t', 'a', 'lk'], ['t', 'a', 'lk'], ['l', 'is', 'ten'], ['▁l', 'is', 'ten'], ['li', 's', 'ten'], ['▁li', 's', 'ten'], ['fuck', '▁y', 'ou'], ['▁fuck', '▁y', 'ou'], ['l', 'iste', 'n'], ['▁l', 'iste', 'n'], ['li', 'ste', 'n'], ['▁li', 'ste', 'n'], ['lis', 'te', 'n'], ['▁lis', 'te', 'n'], ['▁list', 'e', 'n'], ['list', 'e', 'n'], ['l', 'ist', 'en'], ['▁l', 'ist', 'en'], ['li', 'st', 'en'], ['▁li', 'st', 'en'], ['lis', 't', 'en'], ['▁lis', 't', 'en'], ['▁fu', 'ck', '▁you'], ['fu', 'ck', '▁you'], ['▁fuc', 'k', '▁you'], ['▁f', 'uck', '▁you'], ['f', 'uck', '▁you'], ['fuck', '▁yo', 'u'], ['▁fuck', '▁yo', 'u'], ['▁t', 'a', 'l', 'k'], ['t', 'a', 'l', 'k'], ['l', 'i', 's', 'ten'], ['▁l', 'i', 's', 'ten'], ['▁fu', 'ck', '▁y', 'ou'], ['fu', 'ck', '▁y', 'ou'], ['▁fuc', 'k', '▁y', 'ou'], ['▁f', 'uck', '▁y', 'ou'], ['f', 'uck', '▁y', 'ou'], ['l', 'i', 'ste', 'n'], ['▁l', 'i', 'ste', 'n'], ['l', 'is', 'te', 'n'], ['▁l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['▁li', 's', 'te', 'n'], ['l', 'ist', 'e', 'n'], ['▁l', 'ist', 'e', 'n'], ['li', 'st', 'e', 'n'], ['▁li', 'st', 'e', 'n'], ['lis', 't', 'e', 'n'], ['▁lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['▁l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['▁l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['▁li', 's', 't', 'en'], ['▁f', 'u', 'ck', '▁you'], ['f', 'u', 'ck', '▁you'], ['▁fu', 'c', 'k', '▁you'], ['fu', 'c', 'k', '▁you'], ['▁f', 'uc', 'k', '▁you'], ['f', 'uc', 'k', '▁you'], ['fuck', '▁y', 'o', 'u'], ['▁fuck', '▁y', 'o', 'u'], ['▁fu', 'ck', '▁yo', 'u'], ['fu', 'ck', '▁yo', 'u'], ['▁fuc', 'k', '▁yo', 'u'], ['▁f', 'uck', '▁yo', 'u'], ['f', 'uck', '▁yo', 'u'], ['▁f', 'u', 'ck', '▁y', 'ou'], ['f', 'u', 'ck', '▁y', 'ou'], ['▁fu', 'c', 'k', '▁y', 'ou'], ['fu', 'c', 'k', '▁y', 'ou'], ['▁f', 'uc', 'k', '▁y', 'ou'], ['f', 'uc', 'k', '▁y', 'ou'], ['l', 'i', 's', 'te', 'n'], ['▁l', 'i', 's', 'te', 'n'], ['l', 'i', 'st', 'e', 'n'], ['▁l', 'i', 'st', 'e', 'n'], ['l', 'is', 't', 'e', 'n'], ['▁l', 'is', 't', 'e', 'n'], ['li', 's', 't', 'e', 'n'], ['▁li', 's', 't', 'e', 'n'], ['l', 'i', 's', 't', 'en'], ['▁l', 'i', 's', 't', 'en'], ['▁f', 'u', 'c', 'k', '▁you'], ['f', 'u', 'c', 'k', '▁you'], ['▁fu', 'ck', '▁y', 'o', 'u'], ['fu', 'ck', '▁y', 'o', 'u'], ['▁fuc', 'k', '▁y', 'o', 'u'], ['▁f', 'uck', '▁y', 'o', 'u'], ['f', 'uck', '▁y', 'o', 'u'], ['▁f', 'u', 'ck', '▁yo', 'u'], ['f', 'u', 'ck', '▁yo', 'u'], ['▁fu', 'c', 'k', '▁yo', 'u'], ['fu', 'c', 'k', '▁yo', 'u'], ['▁f', 'uc', 'k', '▁yo', 'u'], ['f', 'uc', 'k', '▁yo', 'u'], ['▁f', 'u', 'c', 'k', '▁y', 'ou'], ['f', 'u', 'c', 'k', '▁y', 'ou'], ['l', 'i', 's', 't', 'e', 'n'], ['▁l', 'i', 's', 't', 'e', 'n'], ['▁f', 'u', 'ck', '▁y', 'o', 'u'], ['f', 'u', 'ck', '▁y', 'o', 'u'], ['▁fu', 'c', 'k', '▁y', 'o', 'u'], ['fu', 'c', 'k', '▁y', 'o', 'u'], ['▁f', 'uc', 'k', '▁y', 'o', 'u'], ['f', 'uc', 'k', '▁y', 'o', 'u'], ['▁f', 'u', 'c', 'k', '▁yo', 'u'], ['f', 'u', 'c', 'k', '▁yo', 'u'], ['▁f', 'u', 'c', 'k', '▁y', 'o', 'u'], ['f', 'u', 'c', 'k', '▁y', 'o', 'u']]\n",
      "Final ID Routes: [[18998], [10724], [5063], [33085], [3412, 235273], [3559, 235273], [515, 5547], [702, 5547], [516, 26159], [3586, 26159], [15063, 965], [23966, 965], [474, 2071], [235251, 2071], [32165, 235254], [44003, 235254], [1889, 479], [1701, 479], [235257, 17071], [533, 17071], [34024, 692], [7935, 692], [516, 235257, 235273], [3586, 235257, 235273], [474, 492, 235273], [235251, 492, 235273], [235257, 235252, 5547], [533, 235252, 5547], [474, 235250, 26159], [235251, 235250, 26159], [235257, 502, 965], [533, 502, 965], [515, 235256, 965], [702, 235256, 965], [34024, 597, 507], [7935, 597, 507], [235257, 3671, 235254], [533, 3671, 235254], [515, 2855, 235254], [702, 2855, 235254], [15063, 488, 235254], [23966, 488, 235254], [1889, 235249, 235254], [1701, 235249, 235254], [235257, 694, 479], [533, 694, 479], [515, 490, 479], [702, 490, 479], [15063, 235251, 479], [23966, 235251, 479], [4936, 623, 692], [12819, 623, 692], [79433, 235273, 692], [517, 1870, 692], [235266, 1870, 692], [34024, 10931, 235261], [7935, 10931, 235261], [474, 235250, 235257, 235273], [235251, 235250, 235257, 235273], [235257, 235252, 235256, 965], [533, 235252, 235256, 965], [4936, 623, 597, 507], [12819, 623, 597, 507], [79433, 235273, 597, 507], [517, 1870, 597, 507], [235266, 1870, 597, 507], [235257, 235252, 2855, 235254], [533, 235252, 2855, 235254], [235257, 502, 488, 235254], [533, 502, 488, 235254], [515, 235256, 488, 235254], [702, 235256, 488, 235254], [235257, 694, 235249, 235254], [533, 694, 235249, 235254], [515, 490, 235249, 235254], [702, 490, 235249, 235254], [15063, 235251, 235249, 235254], [23966, 235251, 235249, 235254], [235257, 235252, 490, 479], [533, 235252, 490, 479], [235257, 502, 235251, 479], [533, 502, 235251, 479], [515, 235256, 235251, 479], [702, 235256, 235251, 479], [517, 235261, 623, 692], [235266, 235261, 623, 692], [4936, 235260, 235273, 692], [12819, 235260, 235273, 692], [517, 1669, 235273, 692], [235266, 1669, 235273, 692], [34024, 597, 235253, 235261], [7935, 597, 235253, 235261], [4936, 623, 10931, 235261], [12819, 623, 10931, 235261], [79433, 235273, 10931, 235261], [517, 1870, 10931, 235261], [235266, 1870, 10931, 235261], [517, 235261, 623, 597, 507], [235266, 235261, 623, 597, 507], [4936, 235260, 235273, 597, 507], [12819, 235260, 235273, 597, 507], [517, 1669, 235273, 597, 507], [235266, 1669, 235273, 597, 507], [235257, 235252, 235256, 488, 235254], [533, 235252, 235256, 488, 235254], [235257, 235252, 490, 235249, 235254], [533, 235252, 490, 235249, 235254], [235257, 502, 235251, 235249, 235254], [533, 502, 235251, 235249, 235254], [515, 235256, 235251, 235249, 235254], [702, 235256, 235251, 235249, 235254], [235257, 235252, 235256, 235251, 479], [533, 235252, 235256, 235251, 479], [517, 235261, 235260, 235273, 692], [235266, 235261, 235260, 235273, 692], [4936, 623, 597, 235253, 235261], [12819, 623, 597, 235253, 235261], [79433, 235273, 597, 235253, 235261], [517, 1870, 597, 235253, 235261], [235266, 1870, 597, 235253, 235261], [517, 235261, 623, 10931, 235261], [235266, 235261, 623, 10931, 235261], [4936, 235260, 235273, 10931, 235261], [12819, 235260, 235273, 10931, 235261], [517, 1669, 235273, 10931, 235261], [235266, 1669, 235273, 10931, 235261], [517, 235261, 235260, 235273, 597, 507], [235266, 235261, 235260, 235273, 597, 507], [235257, 235252, 235256, 235251, 235249, 235254], [533, 235252, 235256, 235251, 235249, 235254], [517, 235261, 623, 597, 235253, 235261], [235266, 235261, 623, 597, 235253, 235261], [4936, 235260, 235273, 597, 235253, 235261], [12819, 235260, 235273, 597, 235253, 235261], [517, 1669, 235273, 597, 235253, 235261], [235266, 1669, 235273, 597, 235253, 235261], [517, 235261, 235260, 235273, 10931, 235261], [235266, 235261, 235260, 235273, 10931, 235261], [517, 235261, 235260, 235273, 597, 235253, 235261], [235266, 235261, 235260, 235273, 597, 235253, 235261]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "new_token_candidates = []\n",
    "new_token_id_candidates = []\n",
    "\n",
    "# Define a pattern to filter out unwanted excessive whitespace tokens (e.g., multiple tabs or newlines)\n",
    "invalid_token_pattern = re.compile(r'^[\\t\\n\\r\\f\\v\\s]*$')  # Matches tokens that are made of 3 or more whitespace characters\n",
    "\n",
    "# Initial matching of tokens that start with banned word prefixes\n",
    "for token, token_id in tokenizer.vocab.items():\n",
    "    token_stripped = token.strip()\n",
    "    if len(token_stripped) == 0 or (len(token_stripped) == 1 and token_stripped == \"▁\"): \n",
    "        continue\n",
    "\n",
    "    if token_stripped[0] == \"▁\":\n",
    "        token_stripped = token_stripped[1:]\n",
    "\n",
    "    for banned_word in banned_words:\n",
    "        if banned_word.startswith(token_stripped):\n",
    "            new_token_candidates.append([token])  # Store as a list for consistency\n",
    "            new_token_id_candidates.append([token_id])\n",
    "\n",
    "print(\"Filtered First Candidates:\", new_token_candidates)\n",
    "print(\"Filtered First IDs:\", new_token_id_candidates)\n",
    "\n",
    "# Initialize final routes for fully matched banned words\n",
    "final_routes = []\n",
    "final_id_routes = []\n",
    "for new_token, new_token_id in zip(new_token_candidates, new_token_id_candidates):\n",
    "    new_token_str = \"\".join(new_token).strip()\n",
    "\n",
    "    if new_token_str in banned_words or (new_token_str[0] == \"▁\" and new_token_str[1:] in banned_words):\n",
    "        final_routes.append(new_token)\n",
    "        final_id_routes.append(new_token_id)\n",
    "\n",
    "print(\"(beginning) Final Routes:\", final_routes)\n",
    "print(\"(beginning) Final ID Routes:\", final_id_routes)\n",
    "\n",
    "# Iteratively expand candidates using BFS-like approach\n",
    "while new_token_candidates:\n",
    "    curr_token_candidates = new_token_candidates\n",
    "    curr_token_id_candidates = new_token_id_candidates\n",
    "    new_token_candidates = []\n",
    "    new_token_id_candidates = []\n",
    "\n",
    "    # Iterate over vocabulary and expand each candidate\n",
    "    for token, token_id in tokenizer.vocab.items():\n",
    "        token_stripped = token.strip()\n",
    "        if len(token_stripped) == 0 or (len(token_stripped) == 1 and token_stripped == \"▁\") or re.findall(r\"\\▁{2,100}\", token_stripped):\n",
    "            continue\n",
    "\n",
    "        # Skip tokens that consist of excessive whitespace or control characters\n",
    "        if invalid_token_pattern.match(token_stripped):\n",
    "            continue\n",
    "\n",
    "        for candidate_token, candidate_token_id in zip(curr_token_candidates, curr_token_id_candidates):\n",
    "            curr_token = candidate_token + [token]\n",
    "            curr_token_ids = candidate_token_id + [token_id]\n",
    "\n",
    "            curr_token_str = \"\".join([token.replace(\"▁\", \" \") for token in curr_token]).strip()\n",
    "            # if curr_token_str[0] == \"▁\":\n",
    "            #     curr_token_str = curr_token_str[1:]\n",
    "\n",
    "            # Check if the current token combination matches or is a prefix of any banned word\n",
    "            for banned_word in banned_words:\n",
    "                if curr_token_str == banned_word:\n",
    "                    # Full match found, add to final routes\n",
    "                    final_routes.append(curr_token)\n",
    "                    final_id_routes.append(curr_token_ids)\n",
    "                elif banned_word.startswith(curr_token_str):\n",
    "                    # Partial match, keep expanding this candidate\n",
    "                    new_token_candidates.append(curr_token)\n",
    "                    new_token_id_candidates.append(curr_token_ids)\n",
    "\n",
    "    print(final_routes)\n",
    "\n",
    "print(\"Final Routes:\", final_routes)\n",
    "print(\"Final ID Routes:\", final_id_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen'] [18998]\n",
      "['▁listen'] [10724]\n",
      "['▁talk'] [5063]\n",
      "['talk'] [33085]\n",
      "['▁tal', 'k'] [3412, 235273]\n",
      "['tal', 'k'] [3559, 235273]\n",
      "['li', 'sten'] [515, 5547]\n",
      "['▁li', 'sten'] [702, 5547]\n",
      "['ta', 'lk'] [516, 26159]\n",
      "['▁ta', 'lk'] [3586, 26159]\n",
      "['lis', 'ten'] [15063, 965]\n",
      "['▁lis', 'ten'] [23966, 965]\n",
      "['▁t', 'alk'] [474, 2071]\n",
      "['t', 'alk'] [235251, 2071]\n",
      "['▁liste', 'n'] [32165, 235254]\n",
      "['liste', 'n'] [44003, 235254]\n",
      "['▁list', 'en'] [1889, 479]\n",
      "['list', 'en'] [1701, 479]\n",
      "['l', 'isten'] [235257, 17071]\n",
      "['▁l', 'isten'] [533, 17071]\n",
      "['fuck', '▁you'] [34024, 692]\n",
      "['▁fuck', '▁you'] [7935, 692]\n",
      "['ta', 'l', 'k'] [516, 235257, 235273]\n",
      "['▁ta', 'l', 'k'] [3586, 235257, 235273]\n",
      "['▁t', 'al', 'k'] [474, 492, 235273]\n",
      "['t', 'al', 'k'] [235251, 492, 235273]\n",
      "['l', 'i', 'sten'] [235257, 235252, 5547]\n",
      "['▁l', 'i', 'sten'] [533, 235252, 5547]\n",
      "['▁t', 'a', 'lk'] [474, 235250, 26159]\n",
      "['t', 'a', 'lk'] [235251, 235250, 26159]\n",
      "['l', 'is', 'ten'] [235257, 502, 965]\n",
      "['▁l', 'is', 'ten'] [533, 502, 965]\n",
      "['li', 's', 'ten'] [515, 235256, 965]\n",
      "['▁li', 's', 'ten'] [702, 235256, 965]\n",
      "['fuck', '▁y', 'ou'] [34024, 597, 507]\n",
      "['▁fuck', '▁y', 'ou'] [7935, 597, 507]\n",
      "['l', 'iste', 'n'] [235257, 3671, 235254]\n",
      "['▁l', 'iste', 'n'] [533, 3671, 235254]\n",
      "['li', 'ste', 'n'] [515, 2855, 235254]\n",
      "['▁li', 'ste', 'n'] [702, 2855, 235254]\n",
      "['lis', 'te', 'n'] [15063, 488, 235254]\n",
      "['▁lis', 'te', 'n'] [23966, 488, 235254]\n",
      "['▁list', 'e', 'n'] [1889, 235249, 235254]\n",
      "['list', 'e', 'n'] [1701, 235249, 235254]\n",
      "['l', 'ist', 'en'] [235257, 694, 479]\n",
      "['▁l', 'ist', 'en'] [533, 694, 479]\n",
      "['li', 'st', 'en'] [515, 490, 479]\n",
      "['▁li', 'st', 'en'] [702, 490, 479]\n",
      "['lis', 't', 'en'] [15063, 235251, 479]\n",
      "['▁lis', 't', 'en'] [23966, 235251, 479]\n",
      "['▁fu', 'ck', '▁you'] [4936, 623, 692]\n",
      "['fu', 'ck', '▁you'] [12819, 623, 692]\n",
      "['▁fuc', 'k', '▁you'] [79433, 235273, 692]\n",
      "['▁f', 'uck', '▁you'] [517, 1870, 692]\n",
      "['f', 'uck', '▁you'] [235266, 1870, 692]\n",
      "['fuck', '▁yo', 'u'] [34024, 10931, 235261]\n",
      "['▁fuck', '▁yo', 'u'] [7935, 10931, 235261]\n",
      "['▁t', 'a', 'l', 'k'] [474, 235250, 235257, 235273]\n",
      "['t', 'a', 'l', 'k'] [235251, 235250, 235257, 235273]\n",
      "['l', 'i', 's', 'ten'] [235257, 235252, 235256, 965]\n",
      "['▁l', 'i', 's', 'ten'] [533, 235252, 235256, 965]\n",
      "['▁fu', 'ck', '▁y', 'ou'] [4936, 623, 597, 507]\n",
      "['fu', 'ck', '▁y', 'ou'] [12819, 623, 597, 507]\n",
      "['▁fuc', 'k', '▁y', 'ou'] [79433, 235273, 597, 507]\n",
      "['▁f', 'uck', '▁y', 'ou'] [517, 1870, 597, 507]\n",
      "['f', 'uck', '▁y', 'ou'] [235266, 1870, 597, 507]\n",
      "['l', 'i', 'ste', 'n'] [235257, 235252, 2855, 235254]\n",
      "['▁l', 'i', 'ste', 'n'] [533, 235252, 2855, 235254]\n",
      "['l', 'is', 'te', 'n'] [235257, 502, 488, 235254]\n",
      "['▁l', 'is', 'te', 'n'] [533, 502, 488, 235254]\n",
      "['li', 's', 'te', 'n'] [515, 235256, 488, 235254]\n",
      "['▁li', 's', 'te', 'n'] [702, 235256, 488, 235254]\n",
      "['l', 'ist', 'e', 'n'] [235257, 694, 235249, 235254]\n",
      "['▁l', 'ist', 'e', 'n'] [533, 694, 235249, 235254]\n",
      "['li', 'st', 'e', 'n'] [515, 490, 235249, 235254]\n",
      "['▁li', 'st', 'e', 'n'] [702, 490, 235249, 235254]\n",
      "['lis', 't', 'e', 'n'] [15063, 235251, 235249, 235254]\n",
      "['▁lis', 't', 'e', 'n'] [23966, 235251, 235249, 235254]\n",
      "['l', 'i', 'st', 'en'] [235257, 235252, 490, 479]\n",
      "['▁l', 'i', 'st', 'en'] [533, 235252, 490, 479]\n",
      "['l', 'is', 't', 'en'] [235257, 502, 235251, 479]\n",
      "['▁l', 'is', 't', 'en'] [533, 502, 235251, 479]\n",
      "['li', 's', 't', 'en'] [515, 235256, 235251, 479]\n",
      "['▁li', 's', 't', 'en'] [702, 235256, 235251, 479]\n",
      "['▁f', 'u', 'ck', '▁you'] [517, 235261, 623, 692]\n",
      "['f', 'u', 'ck', '▁you'] [235266, 235261, 623, 692]\n",
      "['▁fu', 'c', 'k', '▁you'] [4936, 235260, 235273, 692]\n",
      "['fu', 'c', 'k', '▁you'] [12819, 235260, 235273, 692]\n",
      "['▁f', 'uc', 'k', '▁you'] [517, 1669, 235273, 692]\n",
      "['f', 'uc', 'k', '▁you'] [235266, 1669, 235273, 692]\n",
      "['fuck', '▁y', 'o', 'u'] [34024, 597, 235253, 235261]\n",
      "['▁fuck', '▁y', 'o', 'u'] [7935, 597, 235253, 235261]\n",
      "['▁fu', 'ck', '▁yo', 'u'] [4936, 623, 10931, 235261]\n",
      "['fu', 'ck', '▁yo', 'u'] [12819, 623, 10931, 235261]\n",
      "['▁fuc', 'k', '▁yo', 'u'] [79433, 235273, 10931, 235261]\n",
      "['▁f', 'uck', '▁yo', 'u'] [517, 1870, 10931, 235261]\n",
      "['f', 'uck', '▁yo', 'u'] [235266, 1870, 10931, 235261]\n",
      "['▁f', 'u', 'ck', '▁y', 'ou'] [517, 235261, 623, 597, 507]\n",
      "['f', 'u', 'ck', '▁y', 'ou'] [235266, 235261, 623, 597, 507]\n",
      "['▁fu', 'c', 'k', '▁y', 'ou'] [4936, 235260, 235273, 597, 507]\n",
      "['fu', 'c', 'k', '▁y', 'ou'] [12819, 235260, 235273, 597, 507]\n",
      "['▁f', 'uc', 'k', '▁y', 'ou'] [517, 1669, 235273, 597, 507]\n",
      "['f', 'uc', 'k', '▁y', 'ou'] [235266, 1669, 235273, 597, 507]\n",
      "['l', 'i', 's', 'te', 'n'] [235257, 235252, 235256, 488, 235254]\n",
      "['▁l', 'i', 's', 'te', 'n'] [533, 235252, 235256, 488, 235254]\n",
      "['l', 'i', 'st', 'e', 'n'] [235257, 235252, 490, 235249, 235254]\n",
      "['▁l', 'i', 'st', 'e', 'n'] [533, 235252, 490, 235249, 235254]\n",
      "['l', 'is', 't', 'e', 'n'] [235257, 502, 235251, 235249, 235254]\n",
      "['▁l', 'is', 't', 'e', 'n'] [533, 502, 235251, 235249, 235254]\n",
      "['li', 's', 't', 'e', 'n'] [515, 235256, 235251, 235249, 235254]\n",
      "['▁li', 's', 't', 'e', 'n'] [702, 235256, 235251, 235249, 235254]\n",
      "['l', 'i', 's', 't', 'en'] [235257, 235252, 235256, 235251, 479]\n",
      "['▁l', 'i', 's', 't', 'en'] [533, 235252, 235256, 235251, 479]\n",
      "['▁f', 'u', 'c', 'k', '▁you'] [517, 235261, 235260, 235273, 692]\n",
      "['f', 'u', 'c', 'k', '▁you'] [235266, 235261, 235260, 235273, 692]\n",
      "['▁fu', 'ck', '▁y', 'o', 'u'] [4936, 623, 597, 235253, 235261]\n",
      "['fu', 'ck', '▁y', 'o', 'u'] [12819, 623, 597, 235253, 235261]\n",
      "['▁fuc', 'k', '▁y', 'o', 'u'] [79433, 235273, 597, 235253, 235261]\n",
      "['▁f', 'uck', '▁y', 'o', 'u'] [517, 1870, 597, 235253, 235261]\n",
      "['f', 'uck', '▁y', 'o', 'u'] [235266, 1870, 597, 235253, 235261]\n",
      "['▁f', 'u', 'ck', '▁yo', 'u'] [517, 235261, 623, 10931, 235261]\n",
      "['f', 'u', 'ck', '▁yo', 'u'] [235266, 235261, 623, 10931, 235261]\n",
      "['▁fu', 'c', 'k', '▁yo', 'u'] [4936, 235260, 235273, 10931, 235261]\n",
      "['fu', 'c', 'k', '▁yo', 'u'] [12819, 235260, 235273, 10931, 235261]\n",
      "['▁f', 'uc', 'k', '▁yo', 'u'] [517, 1669, 235273, 10931, 235261]\n",
      "['f', 'uc', 'k', '▁yo', 'u'] [235266, 1669, 235273, 10931, 235261]\n",
      "['▁f', 'u', 'c', 'k', '▁y', 'ou'] [517, 235261, 235260, 235273, 597, 507]\n",
      "['f', 'u', 'c', 'k', '▁y', 'ou'] [235266, 235261, 235260, 235273, 597, 507]\n",
      "['l', 'i', 's', 't', 'e', 'n'] [235257, 235252, 235256, 235251, 235249, 235254]\n",
      "['▁l', 'i', 's', 't', 'e', 'n'] [533, 235252, 235256, 235251, 235249, 235254]\n",
      "['▁f', 'u', 'ck', '▁y', 'o', 'u'] [517, 235261, 623, 597, 235253, 235261]\n",
      "['f', 'u', 'ck', '▁y', 'o', 'u'] [235266, 235261, 623, 597, 235253, 235261]\n",
      "['▁fu', 'c', 'k', '▁y', 'o', 'u'] [4936, 235260, 235273, 597, 235253, 235261]\n",
      "['fu', 'c', 'k', '▁y', 'o', 'u'] [12819, 235260, 235273, 597, 235253, 235261]\n",
      "['▁f', 'uc', 'k', '▁y', 'o', 'u'] [517, 1669, 235273, 597, 235253, 235261]\n",
      "['f', 'uc', 'k', '▁y', 'o', 'u'] [235266, 1669, 235273, 597, 235253, 235261]\n",
      "['▁f', 'u', 'c', 'k', '▁yo', 'u'] [517, 235261, 235260, 235273, 10931, 235261]\n",
      "['f', 'u', 'c', 'k', '▁yo', 'u'] [235266, 235261, 235260, 235273, 10931, 235261]\n",
      "['▁f', 'u', 'c', 'k', '▁y', 'o', 'u'] [517, 235261, 235260, 235273, 597, 235253, 235261]\n",
      "['f', 'u', 'c', 'k', '▁y', 'o', 'u'] [235266, 235261, 235260, 235273, 597, 235253, 235261]\n"
     ]
    }
   ],
   "source": [
    "for items, ids in zip(final_routes, final_id_routes):\n",
    "    print(items, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Can we talk?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(input_ids: torch.Tensor, max_length: int = 50) -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids.to(device), return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        new_generated_token_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if new_generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat((input_ids, new_generated_token_id.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to listen and help in any way I can. \n",
      "\n",
      "What's on your mind? \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate(input_ids=inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSMProcessor:\n",
    "    def __init__(self, special_token_ids_list: List[List[int]], end_state: int = -1) -> None:\n",
    "        self.end_state = end_state\n",
    "        self.next_state = 1\n",
    "        self.curr_state = 0\n",
    "        self.fsm = {}\n",
    "        self.special_words = []\n",
    "\n",
    "        # Track partial matches\n",
    "        self.partial_match_state = None\n",
    "        self.partial_tokens = []\n",
    "\n",
    "        self.update_group(special_token_ids_list)\n",
    "\n",
    "    def update(self, special_token_ids: List[int]) -> None:\n",
    "        curr_state = 0\n",
    "\n",
    "        for idx, special_token_id in enumerate(special_token_ids):\n",
    "            if curr_state not in self.fsm:\n",
    "                self.fsm[curr_state] = []\n",
    "\n",
    "            state2id = [items[0] for items in self.fsm[curr_state]]\n",
    "            if special_token_id not in state2id:\n",
    "                if idx == len(special_token_ids) - 1:\n",
    "                    self.fsm[curr_state].append([special_token_id, self.end_state])\n",
    "                else:\n",
    "                    self.fsm[curr_state].append([special_token_id, self.next_state])\n",
    "                    curr_state = self.next_state\n",
    "                    self.next_state += 1\n",
    "            else:\n",
    "                for fsm_idx in range(len(self.fsm[curr_state])):\n",
    "                    if special_token_id == self.fsm[curr_state][fsm_idx][0] and idx == len(special_token_ids) - 1:\n",
    "                        self.fsm[curr_state][fsm_idx][1] = self.end_state\n",
    "                        break\n",
    "                    elif special_token_id == self.fsm[curr_state][fsm_idx][0]:\n",
    "                        curr_state = self.fsm[curr_state][fsm_idx][1]\n",
    "                        break\n",
    "\n",
    "    def update_group(self, special_token_ids_list: List[List[int]]) -> None:\n",
    "        for special_token_ids in special_token_ids_list:\n",
    "            self.update(special_token_ids=special_token_ids)\n",
    "\n",
    "    def get_fsm_data(self) -> Dict[str, List[Tuple[int, int]]]:\n",
    "        return self.fsm\n",
    "    \n",
    "    def detect(self, token: int) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if the current token leads to a sensitive sequence.\n",
    "        Updates the current state and returns True if it reaches the end state.\n",
    "        \"\"\"\n",
    "        if self.curr_state in self.fsm:\n",
    "            for transition in self.fsm[self.curr_state]:\n",
    "                if transition[0] == token:\n",
    "                    self.curr_state = transition[1]\n",
    "\n",
    "                    # If the current state reaches the end state\n",
    "                    return self.curr_state == self.end_state\n",
    "        \n",
    "        # If the token does not match, reset the current state\n",
    "        self.curr_state = 0\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[18998, -1],\n",
       "  [10724, -1],\n",
       "  [5063, -1],\n",
       "  [33085, -1],\n",
       "  [3412, 1],\n",
       "  [3559, 2],\n",
       "  [515, 3],\n",
       "  [702, 4],\n",
       "  [516, 5],\n",
       "  [3586, 6],\n",
       "  [15063, 7],\n",
       "  [23966, 8],\n",
       "  [474, 9],\n",
       "  [235251, 10],\n",
       "  [32165, 11],\n",
       "  [44003, 12],\n",
       "  [1889, 13],\n",
       "  [1701, 14],\n",
       "  [235257, 15],\n",
       "  [533, 16],\n",
       "  [34024, 17],\n",
       "  [7935, 18],\n",
       "  [4936, 47],\n",
       "  [12819, 49],\n",
       "  [79433, 51],\n",
       "  [517, 53],\n",
       "  [235266, 55]],\n",
       " 1: [[235273, -1]],\n",
       " 2: [[235273, -1]],\n",
       " 3: [[5547, -1], [235256, 29], [2855, 35], [490, 43]],\n",
       " 4: [[5547, -1], [235256, 30], [2855, 36], [490, 44]],\n",
       " 5: [[26159, -1], [235257, 19]],\n",
       " 6: [[26159, -1], [235257, 20]],\n",
       " 7: [[965, -1], [488, 37], [235251, 45]],\n",
       " 8: [[965, -1], [488, 38], [235251, 46]],\n",
       " 9: [[2071, -1], [492, 21], [235250, 25]],\n",
       " 10: [[2071, -1], [492, 22], [235250, 26]],\n",
       " 11: [[235254, -1]],\n",
       " 12: [[235254, -1]],\n",
       " 13: [[479, -1], [235249, 39]],\n",
       " 14: [[479, -1], [235249, 40]],\n",
       " 15: [[17071, -1], [235252, 23], [502, 27], [3671, 33], [694, 41]],\n",
       " 16: [[17071, -1], [235252, 24], [502, 28], [3671, 34], [694, 42]],\n",
       " 17: [[692, -1], [597, 31], [10931, 57]],\n",
       " 18: [[692, -1], [597, 32], [10931, 58]],\n",
       " 19: [[235273, -1]],\n",
       " 20: [[235273, -1]],\n",
       " 21: [[235273, -1]],\n",
       " 22: [[235273, -1]],\n",
       " 23: [[5547, -1], [235256, 61], [2855, 68], [490, 80]],\n",
       " 24: [[5547, -1], [235256, 62], [2855, 69], [490, 81]],\n",
       " 25: [[26159, -1], [235257, 59]],\n",
       " 26: [[26159, -1], [235257, 60]],\n",
       " 27: [[965, -1], [488, 70], [235251, 82]],\n",
       " 28: [[965, -1], [488, 71], [235251, 83]],\n",
       " 29: [[965, -1], [488, 72], [235251, 84]],\n",
       " 30: [[965, -1], [488, 73], [235251, 85]],\n",
       " 31: [[507, -1], [235253, 98]],\n",
       " 32: [[507, -1], [235253, 99]],\n",
       " 33: [[235254, -1]],\n",
       " 34: [[235254, -1]],\n",
       " 35: [[235254, -1]],\n",
       " 36: [[235254, -1]],\n",
       " 37: [[235254, -1]],\n",
       " 38: [[235254, -1]],\n",
       " 39: [[235254, -1]],\n",
       " 40: [[235254, -1]],\n",
       " 41: [[479, -1], [235249, 74]],\n",
       " 42: [[479, -1], [235249, 75]],\n",
       " 43: [[479, -1], [235249, 76]],\n",
       " 44: [[479, -1], [235249, 77]],\n",
       " 45: [[479, -1], [235249, 78]],\n",
       " 46: [[479, -1], [235249, 79]],\n",
       " 47: [[623, 48], [235260, 90]],\n",
       " 48: [[692, -1], [597, 63], [10931, 100]],\n",
       " 49: [[623, 50], [235260, 92]],\n",
       " 50: [[692, -1], [597, 64], [10931, 101]],\n",
       " 51: [[235273, 52]],\n",
       " 52: [[692, -1], [597, 65], [10931, 102]],\n",
       " 53: [[1870, 54], [235261, 86], [1669, 94]],\n",
       " 54: [[692, -1], [597, 66], [10931, 103]],\n",
       " 55: [[1870, 56], [235261, 88], [1669, 96]],\n",
       " 56: [[692, -1], [597, 67], [10931, 104]],\n",
       " 57: [[235261, -1]],\n",
       " 58: [[235261, -1]],\n",
       " 59: [[235273, -1]],\n",
       " 60: [[235273, -1]],\n",
       " 61: [[965, -1], [488, 111], [235251, 119]],\n",
       " 62: [[965, -1], [488, 112], [235251, 120]],\n",
       " 63: [[507, -1], [235253, 125]],\n",
       " 64: [[507, -1], [235253, 126]],\n",
       " 65: [[507, -1], [235253, 127]],\n",
       " 66: [[507, -1], [235253, 128]],\n",
       " 67: [[507, -1], [235253, 129]],\n",
       " 68: [[235254, -1]],\n",
       " 69: [[235254, -1]],\n",
       " 70: [[235254, -1]],\n",
       " 71: [[235254, -1]],\n",
       " 72: [[235254, -1]],\n",
       " 73: [[235254, -1]],\n",
       " 74: [[235254, -1]],\n",
       " 75: [[235254, -1]],\n",
       " 76: [[235254, -1]],\n",
       " 77: [[235254, -1]],\n",
       " 78: [[235254, -1]],\n",
       " 79: [[235254, -1]],\n",
       " 80: [[479, -1], [235249, 113]],\n",
       " 81: [[479, -1], [235249, 114]],\n",
       " 82: [[479, -1], [235249, 115]],\n",
       " 83: [[479, -1], [235249, 116]],\n",
       " 84: [[479, -1], [235249, 117]],\n",
       " 85: [[479, -1], [235249, 118]],\n",
       " 86: [[623, 87], [235260, 121]],\n",
       " 87: [[692, -1], [597, 105], [10931, 130]],\n",
       " 88: [[623, 89], [235260, 123]],\n",
       " 89: [[692, -1], [597, 106], [10931, 131]],\n",
       " 90: [[235273, 91]],\n",
       " 91: [[692, -1], [597, 107], [10931, 132]],\n",
       " 92: [[235273, 93]],\n",
       " 93: [[692, -1], [597, 108], [10931, 133]],\n",
       " 94: [[235273, 95]],\n",
       " 95: [[692, -1], [597, 109], [10931, 134]],\n",
       " 96: [[235273, 97]],\n",
       " 97: [[692, -1], [597, 110], [10931, 135]],\n",
       " 98: [[235261, -1]],\n",
       " 99: [[235261, -1]],\n",
       " 100: [[235261, -1]],\n",
       " 101: [[235261, -1]],\n",
       " 102: [[235261, -1]],\n",
       " 103: [[235261, -1]],\n",
       " 104: [[235261, -1]],\n",
       " 105: [[507, -1], [235253, 140]],\n",
       " 106: [[507, -1], [235253, 141]],\n",
       " 107: [[507, -1], [235253, 142]],\n",
       " 108: [[507, -1], [235253, 143]],\n",
       " 109: [[507, -1], [235253, 144]],\n",
       " 110: [[507, -1], [235253, 145]],\n",
       " 111: [[235254, -1]],\n",
       " 112: [[235254, -1]],\n",
       " 113: [[235254, -1]],\n",
       " 114: [[235254, -1]],\n",
       " 115: [[235254, -1]],\n",
       " 116: [[235254, -1]],\n",
       " 117: [[235254, -1]],\n",
       " 118: [[235254, -1]],\n",
       " 119: [[479, -1], [235249, 138]],\n",
       " 120: [[479, -1], [235249, 139]],\n",
       " 121: [[235273, 122]],\n",
       " 122: [[692, -1], [597, 136], [10931, 146]],\n",
       " 123: [[235273, 124]],\n",
       " 124: [[692, -1], [597, 137], [10931, 147]],\n",
       " 125: [[235261, -1]],\n",
       " 126: [[235261, -1]],\n",
       " 127: [[235261, -1]],\n",
       " 128: [[235261, -1]],\n",
       " 129: [[235261, -1]],\n",
       " 130: [[235261, -1]],\n",
       " 131: [[235261, -1]],\n",
       " 132: [[235261, -1]],\n",
       " 133: [[235261, -1]],\n",
       " 134: [[235261, -1]],\n",
       " 135: [[235261, -1]],\n",
       " 136: [[507, -1], [235253, 148]],\n",
       " 137: [[507, -1], [235253, 149]],\n",
       " 138: [[235254, -1]],\n",
       " 139: [[235254, -1]],\n",
       " 140: [[235261, -1]],\n",
       " 141: [[235261, -1]],\n",
       " 142: [[235261, -1]],\n",
       " 143: [[235261, -1]],\n",
       " 144: [[235261, -1]],\n",
       " 145: [[235261, -1]],\n",
       " 146: [[235261, -1]],\n",
       " 147: [[235261, -1]],\n",
       " 148: [[235261, -1]],\n",
       " 149: [[235261, -1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor = FSMProcessor(special_token_ids_list=final_id_routes)\n",
    "fsm_processor.get_fsm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_fsm_filter(\n",
    "    input_ids: torch.Tensor,\n",
    "    fsm_processor: FSMProcessor,\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_history = {}\n",
    "    past_key_values = None\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_length:\n",
    "        steps += 1\n",
    "\n",
    "        # Generate new token with kv cache\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Update kv cache\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Check if there are already masked tokens at the current step\n",
    "        if steps in masked_tokens_history:\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "        else:\n",
    "            masked_tokens_history[steps] = set()\n",
    "\n",
    "        # Decode the generated token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        combined_ids = torch.cat((input_ids, torch.tensor([[generated_token_id]], device=input_ids.device)), dim=-1)\n",
    "\n",
    "        # Check FSM for sensitive sequences\n",
    "        if fsm_processor.detect(generated_token_id):\n",
    "            # Detected a sensitive sequence, initiate rollback\n",
    "            rollback_length = fsm_processor.partial_match_state + 1 if fsm_processor.partial_match_state is not None else 1\n",
    "            steps = steps - rollback_length + 1\n",
    "            rollbacks_ids = combined_ids[:, :-rollback_length]\n",
    "            input_ids = rollbacks_ids\n",
    "            print(f\"Rollback detected. Rolling back from step {steps + rollback_length} to step {steps}\")\n",
    "\n",
    "            # Reset FSM state\n",
    "            fsm_processor.curr_state = 0\n",
    "            fsm_processor.partial_match_state = None\n",
    "\n",
    "            # Reset past_key_values when rolling back\n",
    "            past_key_values = None\n",
    "\n",
    "            # Recalculate logits based on rolled-back sequence\n",
    "            outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # Mask the first token of the sensitive sequence\n",
    "            first_token_id = generated_token_id\n",
    "            print(f\"Masking token id: {first_token_id}, Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "\n",
    "            # Update the historical mask list to record the token at this step\n",
    "            masked_tokens_history[steps].add(first_token_id)\n",
    "\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "\n",
    "            # Generate the token again after masking\n",
    "            generated_token_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        # Update input_ids with the generated token\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([[generated_token_id]], device=input_ids.device)), dim=1)\n",
    "\n",
    "        print(f\"Step {steps}: ID: {generated_token_id} Generated token: {tokenizer.decode(generated_token_id)}\")\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 2: ID: 235285 Generated token: I\n",
      "Step 3: ID: 235303 Generated token: '\n",
      "Step 4: ID: 235262 Generated token: m\n",
      "Step 5: ID: 1517 Generated token:  here\n",
      "Step 6: ID: 577 Generated token:  to\n",
      "Rollback detected. Rolling back from step 8 to step 7\n",
      "Masking token id: 10724, Masking token:  listen\n",
      "Step 7: ID: 1707 Generated token:  help\n",
      "Step 8: ID: 692 Generated token:  you\n",
      "Step 9: ID: 675 Generated token:  with\n",
      "Step 10: ID: 9550 Generated token:  whatever\n",
      "Step 11: ID: 692 Generated token:  you\n",
      "Step 12: ID: 1476 Generated token:  need\n",
      "Step 13: ID: 235265 Generated token: .\n",
      "Step 14: ID: 235248 Generated token:  \n",
      "Step 15: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 16: ID: 5958 Generated token: Please\n",
      "Step 17: ID: 3337 Generated token:  tell\n",
      "Step 18: ID: 682 Generated token:  me\n",
      "Step 19: ID: 1212 Generated token:  what\n",
      "Step 20: ID: 235303 Generated token: '\n",
      "Step 21: ID: 235256 Generated token: s\n",
      "Step 22: ID: 611 Generated token:  on\n",
      "Step 23: ID: 861 Generated token:  your\n",
      "Step 24: ID: 3403 Generated token:  mind\n",
      "Step 25: ID: 235265 Generated token: .\n",
      "Step 26: ID: 44416 Generated token:  😊\n",
      "Step 27: ID: 108 Generated token: \n",
      "\n",
      "Step 28: ID: 107 Generated token: <end_of_turn>\n",
      "Step 29: ID: 1 Generated token: <eos>\n",
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to help you with whatever you need. \n",
      "\n",
      "Please tell me what's on your mind. 😊\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_fsm_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    fsm_processor=fsm_processor,\n",
    "    max_length=50,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 2: ID: 235285 Generated token: I\n",
      "Step 3: ID: 235303 Generated token: '\n",
      "Step 4: ID: 235262 Generated token: m\n",
      "Step 5: ID: 1517 Generated token:  here\n",
      "Step 6: ID: 577 Generated token:  to\n",
      "Rollback detected. Rolling back from step 8 to step 7\n",
      "Masking token id: 10724, Masking token:  listen\n",
      "Step 7: ID: 1707 Generated token:  help\n",
      "Step 8: ID: 692 Generated token:  you\n",
      "Step 9: ID: 675 Generated token:  with\n",
      "Step 10: ID: 9550 Generated token:  whatever\n",
      "Step 11: ID: 692 Generated token:  you\n",
      "Step 12: ID: 1476 Generated token:  need\n",
      "Step 13: ID: 235265 Generated token: .\n",
      "Step 14: ID: 235248 Generated token:  \n",
      "Step 15: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 16: ID: 5958 Generated token: Please\n",
      "Step 17: ID: 3337 Generated token:  tell\n",
      "Step 18: ID: 682 Generated token:  me\n",
      "Step 19: ID: 1212 Generated token:  what\n",
      "Step 20: ID: 235303 Generated token: '\n",
      "Step 21: ID: 235256 Generated token: s\n",
      "Step 22: ID: 611 Generated token:  on\n",
      "Step 23: ID: 861 Generated token:  your\n",
      "Step 24: ID: 3403 Generated token:  mind\n",
      "Step 25: ID: 235265 Generated token: .\n",
      "Step 26: ID: 44416 Generated token:  😊\n",
      "Step 27: ID: 108 Generated token: \n",
      "\n",
      "Step 28: ID: 107 Generated token: <end_of_turn>\n",
      "Step 29: ID: 1 Generated token: <eos>\n",
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to help you with whatever you need. \n",
      "\n",
      "Please tell me what's on your mind. 😊\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "response = custom_generate_with_fsm_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    fsm_processor=fsm_processor,\n",
    "    max_length=50,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to help you with whatever you need. \n",
      "\n",
      "Please tell me what's on your mind. 😊\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
