{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"./models/google--gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Can we talk?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to listen and offer support. \n",
      "\n",
      "What'\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(input_ids: torch.Tensor, max_length: int = 50) -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        new_generated_token = torch.argmax(logits, dim=-1)\n",
    "        input_ids = torch.cat((input_ids, new_generated_token.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we talk?\n",
      "\n",
      "I'm here to listen and offer support. \n",
      "\n",
      "What's on your mind? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate(input_ids=inputs.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Generate token\n",
    "        generated_token = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Check for each special word\n",
    "        for special_word in special_words:\n",
    "            if special_word in combined_text:\n",
    "                # Tokenize the special word\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Rollback to before the special word\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"{combined_ids.shape[1]} -> {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recompute logits after rollback\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recompute logits based on rolled-back input\n",
    "                print(logits.shape)\n",
    "\n",
    "                # Mask only the first token of the special word\n",
    "                first_token_id = special_word_tokenized[0, 0]\n",
    "                print(f\"Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "                logits[:, first_token_id] = -float(\"inf\")  # Apply mask to the logits\n",
    "\n",
    "        # Generate new token\n",
    "        new_generated_token = torch.argmax(logits, dim=-1)\n",
    "        print(f\"Generated token: {tokenizer.decode(new_generated_token)}\")\n",
    "        input_ids = torch.cat((input_ids, new_generated_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Generated token:  listen\n",
      "Generated token:  and\n",
      "Generated token:  offer\n",
      "Generated token:  support\n",
      "Generated token: .\n",
      "Generated token:  \n",
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: What\n",
      "Generated token: '\n",
      "Generated token: s\n",
      "Generated token:  on\n",
      "Generated token:  your\n",
      "Generated token:  mind\n",
      "Generated token: ?\n",
      "Can we talk?\n",
      "\n",
      "I'm here to listen and offer support. \n",
      "\n",
      "What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"not sure\", \"fan\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  listen\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "13 -> 12\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  and\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "14 -> 13\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  offer\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "15 -> 14\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  support\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "16 -> 15\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: .\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "17 -> 16\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  \n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "18 -> 17\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: \n",
      "\n",
      "\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "19 -> 18\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: What\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "20 -> 19\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: '\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "21 -> 20\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: s\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "22 -> 21\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  on\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "23 -> 22\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  your\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "24 -> 23\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  mind\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "25 -> 24\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: ?\n",
      "Can we talk?\n",
      "\n",
      "I'm here to listen and offer support. \n",
      "\n",
      "What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"listen\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Detect special word:  listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token:  listen\n",
      "Generated token:  help\n",
      "Generated token:  you\n",
      "Generated token:  with\n",
      "Generated token:  whatever\n",
      "Generated token:  you\n",
      "Generated token:  need\n",
      "Generated token: .\n",
      "Generated token:  \n",
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: Please\n",
      "Generated token:  tell\n",
      "Generated token:  me\n",
      "Generated token:  what\n",
      "Generated token: '\n",
      "Can we talk?\n",
      "\n",
      "I'm here to help you with whatever you need. \n",
      "\n",
      "Please tell me what'\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"listen\", \" listen\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18998]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"listen\", return_tensors=\"pt\", add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_history = {}\n",
    "\n",
    "    for step in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Check if there are already masked tokens at the current step\n",
    "        if step in masked_tokens_history:\n",
    "            for token_id in masked_tokens_history[step]:\n",
    "                logits[:, token_id] = -float(\"inf\")  # Mask previously invalid tokens\n",
    "\n",
    "        # Generate the token\n",
    "        generated_token = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Check for each sensitive word\n",
    "        for special_word in special_words:\n",
    "            if special_word in combined_text:\n",
    "                # Convert the sensitive word into tokens\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Roll back to before the sensitive word\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"Roll back from {combined_ids.shape[1]} to {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recalculate logits based on the rolled-back sequence\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recalculate logits based on rolled-back input\n",
    "\n",
    "                # Only mask the first token of the sensitive word\n",
    "                first_token_id = special_word_tokenized[0, 0]\n",
    "                print(f\"Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "                logits[:, first_token_id] = -float(\"inf\")  # Mask the first token of the sensitive word\n",
    "\n",
    "                # Update the historical mask list to record the token at this step\n",
    "                if step not in masked_tokens_history:\n",
    "                    masked_tokens_history[step] = set()\n",
    "                masked_tokens_history[step].add(first_token_id)\n",
    "\n",
    "        # Generate the next token\n",
    "        new_generated_token = torch.argmax(logits, dim=-1)\n",
    "        print(f\"Generated token: {tokenizer.decode(new_generated_token)}\")\n",
    "        input_ids = torch.cat((input_ids, new_generated_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
