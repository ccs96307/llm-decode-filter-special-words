{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.96it/s]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"./models/google--gemma-2-2b-it\"\n",
    "# pretrained_model_name_or_path = \"openai-community/gpt2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Can we talk?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(input_ids: torch.Tensor, max_length: int = 50) -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        new_generated_token_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if new_generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat((input_ids, new_generated_token_id.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcustom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(input_ids, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_generate\u001b[39m(input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, max_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Generate new tokens\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m      7\u001b[0m         new_generated_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gemma2/modeling_gemma2.py:994\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    992\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1008\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gemma2/modeling_gemma2.py:843\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    832\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    833\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    834\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m         cache_position,\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gemma2/modeling_gemma2.py:600\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    598\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    599\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 600\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    602\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gemma2/modeling_gemma2.py:204\u001b[0m, in \u001b[0;36mGemma2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(custom_generate(input_ids=inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Words Filter Decoding V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Generate token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token:\n",
    "            break\n",
    "\n",
    "        # Check for each special word\n",
    "        for special_word in special_words:\n",
    "            if special_word in combined_text:\n",
    "                # Tokenize the special word\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Rollback to before the special word\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"{combined_ids.shape[1]} -> {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recompute logits after rollback\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recompute logits based on rolled-back input\n",
    "                print(logits.shape)\n",
    "\n",
    "                # Mask only the first token of the special word\n",
    "                first_token_id = special_word_tokenized[0, 0]\n",
    "                print(f\"Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "                logits[:, first_token_id] = -float(\"inf\")  # Apply mask to the logits\n",
    "\n",
    "        # Generate new token\n",
    "        new_generated_token = torch.argmax(logits, dim=-1)\n",
    "        print(f\"Generated token: {tokenizer.decode(new_generated_token)}\")\n",
    "        input_ids = torch.cat((input_ids, new_generated_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  listen\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "13 -> 12\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  and\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "14 -> 13\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  help\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "15 -> 14\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  in\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "16 -> 15\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  any\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "17 -> 16\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  way\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "18 -> 17\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  I\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "19 -> 18\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  can\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "20 -> 19\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: .\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "21 -> 20\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  \n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "22 -> 21\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: \n",
      "\n",
      "\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "23 -> 22\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: What\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "24 -> 23\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: '\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "25 -> 24\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: s\n",
      "Can we talk?\n",
      "\n",
      "I'm here to listen and help in any way I can. \n",
      "\n",
      "What's\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"not sure\", \"fan\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  listen\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "13 -> 12\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  and\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "14 -> 13\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  help\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "15 -> 14\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  in\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "16 -> 15\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  any\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "17 -> 16\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  way\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "18 -> 17\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  I\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "19 -> 18\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  can\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "20 -> 19\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: .\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "21 -> 20\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token:  \n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "22 -> 21\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: \n",
      "\n",
      "\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "23 -> 22\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: What\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "24 -> 23\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: '\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "25 -> 24\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Generated token: s\n",
      "Can we talk?\n",
      "\n",
      "I'm here to listen and help in any way I can. \n",
      "\n",
      "What's\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"listen\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: I\n",
      "Generated token: '\n",
      "Generated token: m\n",
      "Generated token:  here\n",
      "Generated token:  to\n",
      "Detect special word: listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token: listen\n",
      "Detect special word:  listen\n",
      " Start roll-back process...\n",
      "12 -> 11\n",
      "torch.Size([1, 256000])\n",
      "Masking token:  listen\n",
      "Generated token:  help\n",
      "Generated token:  you\n",
      "Generated token:  with\n",
      "Generated token:  whatever\n",
      "Generated token:  you\n",
      "Generated token:  need\n",
      "Generated token: .\n",
      "Generated token:  \n",
      "Generated token: \n",
      "\n",
      "\n",
      "Generated token: Please\n",
      "Generated token:  tell\n",
      "Generated token:  me\n",
      "Generated token:  what\n",
      "Generated token: '\n",
      "Can we talk?\n",
      "\n",
      "I'm here to help you with whatever you need. \n",
      "\n",
      "Please tell me what'\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"listen\", \" listen\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18998]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"listen\", return_tensors=\"pt\", add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter_v2(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_history = {}\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    while step < max_length:\n",
    "        step += 1\n",
    "\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Check if there are already masked tokens at the current step\n",
    "        if step in masked_tokens_history:\n",
    "            for token_id in masked_tokens_history[step]:\n",
    "                logits[:, token_id] = -float(\"inf\")  # Mask previously invalid tokens\n",
    "\n",
    "        # Generate the token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Check for each sensitive word\n",
    "        need_to_generate_again = False\n",
    "\n",
    "        for special_word in special_words:\n",
    "            if special_word in combined_text:\n",
    "                # Convert the sensitive word into tokens\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Roll back to before the sensitive word\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"Roll back from {combined_ids.shape[1]} to {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recalculate logits based on the rolled-back sequence\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recalculate logits based on rolled-back input\n",
    "\n",
    "                # Only mask the first token of the sensitive word\n",
    "                first_token_id = special_word_tokenized[0, 0]\n",
    "                print(f\"Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "                logits[:, first_token_id] = -float(\"inf\")  # Mask the first token of the sensitive word\n",
    "\n",
    "                # Update the historical mask list to record the token at this step\n",
    "                if step not in masked_tokens_history:\n",
    "                    masked_tokens_history[step] = set()\n",
    "                masked_tokens_history[step].add(first_token_id)\n",
    "\n",
    "                need_to_generate_again = True\n",
    "\n",
    "        # Generate the next token\n",
    "        if need_to_generate_again:\n",
    "            generated_token_id = torch.argmax(logits, dim=-1)\n",
    "            print(f\"Generated token: {tokenizer.decode(generated_token_id)}\")\n",
    "\n",
    "        input_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=1)\n",
    "\n",
    "    print()\n",
    "    from pprint import pprint\n",
    "    pprint(masked_tokens_history)\n",
    "    print()\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "Detect special word: not sure\n",
      " Start roll-back process...\n",
      "Roll back from 10 to 8\n",
      "Masking token: not\n",
      "Generated token:  not\n",
      "\n",
      "{6: {tensor(1662)},\n",
      " 7: {tensor(1662)},\n",
      " 8: {tensor(1662)},\n",
      " 9: {tensor(1662)},\n",
      " 10: {tensor(1662)},\n",
      " 11: {tensor(1662)},\n",
      " 12: {tensor(1662)},\n",
      " 13: {tensor(1662)},\n",
      " 14: {tensor(1662)},\n",
      " 15: {tensor(1662)},\n",
      " 16: {tensor(1662)},\n",
      " 17: {tensor(1662)},\n",
      " 18: {tensor(1662)},\n",
      " 19: {tensor(1662)},\n",
      " 20: {tensor(1662)},\n",
      " 21: {tensor(1662)},\n",
      " 22: {tensor(1662)},\n",
      " 23: {tensor(1662)},\n",
      " 24: {tensor(1662)},\n",
      " 25: {tensor(1662)},\n",
      " 26: {tensor(1662)},\n",
      " 27: {tensor(1662)},\n",
      " 28: {tensor(1662)},\n",
      " 29: {tensor(1662)},\n",
      " 30: {tensor(1662)},\n",
      " 31: {tensor(1662)},\n",
      " 32: {tensor(1662)},\n",
      " 33: {tensor(1662)},\n",
      " 34: {tensor(1662)},\n",
      " 35: {tensor(1662)},\n",
      " 36: {tensor(1662)},\n",
      " 37: {tensor(1662)},\n",
      " 38: {tensor(1662)},\n",
      " 39: {tensor(1662)},\n",
      " 40: {tensor(1662)},\n",
      " 41: {tensor(1662)},\n",
      " 42: {tensor(1662)},\n",
      " 43: {tensor(1662)},\n",
      " 44: {tensor(1662)},\n",
      " 45: {tensor(1662)},\n",
      " 46: {tensor(1662)},\n",
      " 47: {tensor(1662)},\n",
      " 48: {tensor(1662)},\n",
      " 49: {tensor(1662)},\n",
      " 50: {tensor(1662)},\n",
      " 51: {tensor(1662)},\n",
      " 52: {tensor(1662)},\n",
      " 53: {tensor(1662)},\n",
      " 54: {tensor(1662)},\n",
      " 55: {tensor(1662)},\n",
      " 56: {tensor(1662)},\n",
      " 57: {tensor(1662)},\n",
      " 58: {tensor(1662)},\n",
      " 59: {tensor(1662)},\n",
      " 60: {tensor(1662)},\n",
      " 61: {tensor(1662)},\n",
      " 62: {tensor(1662)},\n",
      " 63: {tensor(1662)},\n",
      " 64: {tensor(1662)},\n",
      " 65: {tensor(1662)},\n",
      " 66: {tensor(1662)},\n",
      " 67: {tensor(1662)},\n",
      " 68: {tensor(1662)},\n",
      " 69: {tensor(1662)},\n",
      " 70: {tensor(1662)},\n",
      " 71: {tensor(1662)},\n",
      " 72: {tensor(1662)},\n",
      " 73: {tensor(1662)},\n",
      " 74: {tensor(1662)},\n",
      " 75: {tensor(1662)},\n",
      " 76: {tensor(1662)},\n",
      " 77: {tensor(1662)},\n",
      " 78: {tensor(1662)},\n",
      " 79: {tensor(1662)},\n",
      " 80: {tensor(1662)},\n",
      " 81: {tensor(1662)},\n",
      " 82: {tensor(1662)},\n",
      " 83: {tensor(1662)},\n",
      " 84: {tensor(1662)},\n",
      " 85: {tensor(1662)},\n",
      " 86: {tensor(1662)},\n",
      " 87: {tensor(1662)},\n",
      " 88: {tensor(1662)},\n",
      " 89: {tensor(1662)},\n",
      " 90: {tensor(1662)},\n",
      " 91: {tensor(1662)},\n",
      " 92: {tensor(1662)},\n",
      " 93: {tensor(1662)},\n",
      " 94: {tensor(1662)},\n",
      " 95: {tensor(1662)},\n",
      " 96: {tensor(1662)},\n",
      " 97: {tensor(1662)},\n",
      " 98: {tensor(1662)},\n",
      " 99: {tensor(1662)},\n",
      " 100: {tensor(1662)}}\n",
      "\n",
      "Can we talk?\n",
      "\n",
      "I'm not\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter_v2(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"not sure\", \"  not\"],\n",
    "    max_length=100,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter_v3(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_history = {}\n",
    "    past_key_values = None\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_length:\n",
    "        steps += 1\n",
    "\n",
    "        # Generate new token with kv cache\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Update kv cache\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Check if there are already masked tokens at the current steps\n",
    "        if steps in masked_tokens_history:\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "        else:\n",
    "            masked_tokens_history[steps] = set()\n",
    "\n",
    "        # Decode the generated token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(steps, combined_text)\n",
    "\n",
    "        for special_word in special_words:\n",
    "            special_word_length = len(special_word)\n",
    "            if special_word in combined_text[-special_word_length:]:\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Roll back to before sensitive word\n",
    "                steps = steps - special_word_length + 1\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"Roll back from {combined_ids.shape[1]} to {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recalculate logits based on the rolled-back sequence\n",
    "                # Reset past_key_values when rolling back\n",
    "                past_key_values = None\n",
    "\n",
    "                print(input_ids.shape)\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recalculate logits based on rolled-back input\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                # Only mask the first token of the sensitive word\n",
    "                first_token_id = int(special_word_tokenized[0, 0])\n",
    "                print(f\"Masking token id: {first_token_id}, Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "\n",
    "                # Update the historical mask list to record the token at this step\n",
    "                masked_tokens_history[steps].add(first_token_id)\n",
    "\n",
    "                for masked_token_id in masked_tokens_history[steps]:\n",
    "                    logits[:, masked_token_id] = -float(\"inf\")\n",
    "\n",
    "                # Generate the token again after masking\n",
    "                generated_token_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Update input_ids with the generated token\n",
    "        input_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=1)\n",
    "\n",
    "        print()\n",
    "        from pprint import pprint\n",
    "        pprint(masked_tokens_history)\n",
    "        print()\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(custom_generate_with_special_words_filter_v3(\n\u001b[0;32m----> 2\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids,\n\u001b[1;32m      3\u001b[0m     special_words\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalk?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot sure\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      5\u001b[0m ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_special_words_filter_v3(\n",
    "    input_ids=inputs.input_ids,\n",
    "    special_words=[\"talk\", \"talk?\", \"not sure\"],\n",
    "    max_length=20,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_special_words_filter_fsm(\n",
    "    input_ids: torch.Tensor,\n",
    "    special_words: List[str],\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_states = {}\n",
    "    past_key_values = None\n",
    "    steps = 0\n",
    "    decode_state = 0\n",
    "\n",
    "    special_tokens_list = [\n",
    "        tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids[0].tolist()\n",
    "        for special_word in special_words\n",
    "    ]\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    while steps < max_length:\n",
    "        steps += 1\n",
    "\n",
    "        # Generate new token with kv cache\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Update kv cache\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Check if there are already masked tokens at the current steps\n",
    "        if steps not in masked_tokens_history:\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "        else:\n",
    "            masked_tokens_history[steps] = set()\n",
    "\n",
    "        # Decode the generated token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1)\n",
    "        combined_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=-1)\n",
    "        combined_text = tokenizer.decode(combined_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(steps, combined_text)\n",
    "\n",
    "        for special_word in special_words:\n",
    "            special_word_length = len(special_word)\n",
    "            if special_word in combined_text[-special_word_length:]:\n",
    "                special_word_tokenized = tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "                special_word_length = special_word_tokenized.shape[1]\n",
    "                print(f\"Detect special word: {special_word}\\n Start roll-back process...\")\n",
    "\n",
    "                # Roll back to before sensitive word\n",
    "                steps = steps - special_word_length + 1\n",
    "                rollbacks_ids = combined_ids[:, :-special_word_length]\n",
    "                input_ids = rollbacks_ids\n",
    "                print(f\"Roll back from {combined_ids.shape[1]} to {rollbacks_ids.shape[1]}\")\n",
    "\n",
    "                # Recalculate logits based on the rolled-back sequence\n",
    "                # Reset past_key_values when rolling back\n",
    "                past_key_values = None\n",
    "\n",
    "                print(input_ids.shape)\n",
    "                outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "                logits = outputs.logits[:, -1, :]  # Recalculate logits based on rolled-back input\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                # Only mask the first token of the sensitive word\n",
    "                first_token_id = int(special_word_tokenized[0, 0])\n",
    "                print(f\"Masking token id: {first_token_id}, Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "\n",
    "                # Update the historical mask list to record the token at this step\n",
    "                masked_tokens_history[steps].add(first_token_id)\n",
    "\n",
    "                for masked_token_id in masked_tokens_history[steps]:\n",
    "                    logits[:, masked_token_id] = -float(\"inf\")\n",
    "\n",
    "                # Generate the token again after masking\n",
    "                generated_token_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Update input_ids with the generated token\n",
    "        input_ids = torch.cat((input_ids, generated_token_id.unsqueeze(0)), dim=1)\n",
    "\n",
    "        print()\n",
    "        from pprint import pprint\n",
    "        pprint(masked_tokens_history)\n",
    "        print()\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'you', 'dad']\n",
      "['fuck', 'your', 'mother']\n",
      "['fuck', 'you']\n",
      "{0: [['fuck', 1]],\n",
      " 1: [['you', -1], ['your', 3]],\n",
      " 2: [['dad', -1]],\n",
      " 3: [['mother', -1]]}\n"
     ]
    }
   ],
   "source": [
    "special_words = [\"fuck you dad\", \"fuck your mother\", \"fuck you\"]\n",
    "\n",
    "special_tokens_list = [\n",
    "    tokenizer(special_word, return_tensors=\"pt\", add_special_tokens=False).input_ids[0].tolist()\n",
    "    for special_word in special_words\n",
    "]\n",
    "\n",
    "special_tokens_list = [items.split() for items in special_words]\n",
    "\n",
    "end_state = -1\n",
    "next_state = 1\n",
    "fsm = {}\n",
    "\n",
    "for special_tokens in special_tokens_list:\n",
    "    print(special_tokens)\n",
    "    curr_state = 0\n",
    "\n",
    "    for idx, special_token in enumerate(special_tokens):\n",
    "        if curr_state not in fsm:\n",
    "            fsm[curr_state] = []\n",
    "\n",
    "        state2tokens = [items[0] for items in fsm[curr_state]]\n",
    "        if special_token not in state2tokens:\n",
    "            # END\n",
    "            if idx == len(special_tokens) - 1:\n",
    "                fsm[curr_state].append([special_token, end_state])\n",
    "            else:\n",
    "                fsm[curr_state].append([special_token, next_state])\n",
    "                curr_state = next_state\n",
    "                next_state += 1\n",
    "        else:\n",
    "            for fsm_idx in range(len(fsm[curr_state])):\n",
    "                if special_token == fsm[curr_state][fsm_idx][0] and idx == len(special_tokens) - 1:\n",
    "                    fsm[curr_state][fsm_idx][1] = end_state\n",
    "                    break\n",
    "                elif special_token == fsm[curr_state][fsm_idx][0]:\n",
    "                    curr_state = fsm[curr_state][fsm_idx][1]\n",
    "                    break\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(fsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSMProcessor:\n",
    "    def __init__(self, sepcial_tokens_list: List[List[str]], end_state: int = -1) -> None:\n",
    "        self.end_state = end_state\n",
    "        self.next_state = 1\n",
    "        self.curr_state = 0\n",
    "        self.fsm = {}\n",
    "        self.special_words = []\n",
    "\n",
    "        # Track partial matches\n",
    "        self.partial_match_state = None\n",
    "        self.partial_tokens = []\n",
    "\n",
    "        self.update_group(sepcial_tokens_list)\n",
    "\n",
    "    def update(self, special_tokens: List[int]) -> None:\n",
    "        curr_state = 0\n",
    "\n",
    "        for idx, special_token in enumerate(special_tokens):\n",
    "            if curr_state not in self.fsm:\n",
    "                self.fsm[curr_state] = []\n",
    "\n",
    "            state2tokens = [items[0] for items in self.fsm[curr_state]]\n",
    "            if special_token not in state2tokens:\n",
    "                if idx == len(special_tokens) - 1:\n",
    "                    self.fsm[curr_state].append([special_token, self.end_state])\n",
    "                else:\n",
    "                    self.fsm[curr_state].append([special_token, self.next_state])\n",
    "                    curr_state = self.next_state\n",
    "                    self.next_state += 1\n",
    "            else:\n",
    "                for fsm_idx in range(len(self.fsm[curr_state])):\n",
    "                    if special_token == self.fsm[curr_state][fsm_idx][0] and idx == len(special_tokens) - 1:\n",
    "                        self.fsm[curr_state][fsm_idx][1] = self.end_state\n",
    "                        break\n",
    "                    elif special_token == self.fsm[curr_state][fsm_idx][0]:\n",
    "                        curr_state = self.fsm[curr_state][fsm_idx][1]\n",
    "                        break\n",
    "\n",
    "    def update_group(self, special_tokens_list: List[List[int]]) -> None:\n",
    "        for special_tokens in special_tokens_list:\n",
    "            self.update(special_tokens=special_tokens)\n",
    "\n",
    "    def get_fsm_data(self) -> Dict[str, List[Tuple[int, int]]]:\n",
    "        return self.fsm\n",
    "    \n",
    "    def detect(self, token: int) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if the current token leads to a sensitive sequence.\n",
    "        Updates the current state and returns True if it reaches the end state.\n",
    "        \"\"\"\n",
    "        if self.curr_state in self.fsm:\n",
    "            for transition in self.fsm[self.curr_state]:\n",
    "                if transition[0] == token:\n",
    "                    self.curr_state = transition[1]\n",
    "\n",
    "                    # If the current state reaches the end state\n",
    "                    return self.curr_state == self.end_state\n",
    "        \n",
    "        # If the token does not match, reset the current state\n",
    "        self.curr_state = 0\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm_processor = FSMProcessor(special_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuck', 'you', 'dad'], ['fuck', 'your', 'mother'], ['fuck', 'you']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm_processor.update(['Thank', 'you', 'dad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [['fuck', 1], ['Thank', 4]],\n",
       " 1: [['you', -1], ['your', 3]],\n",
       " 2: [['dad', -1]],\n",
       " 3: [['mother', -1]],\n",
       " 4: [['you', 5]],\n",
       " 5: [['dad', -1]]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor.get_fsm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor.detect(\"Thank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor.detect(\"you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor.detect(\"dad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement FSMProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSMProcessor:\n",
    "    def __init__(self, special_words: List[str], end_state: int = -1) -> None:\n",
    "        self.end_state = end_state\n",
    "        self.next_state = 1\n",
    "        self.curr_state = 0\n",
    "        self.fsm = {}\n",
    "        self.special_words = special_words\n",
    "\n",
    "        # Track partial matches\n",
    "        self.partial_match_state = None\n",
    "        self.partial_tokens = []\n",
    "\n",
    "        self.update_group(special_words=special_words)\n",
    "\n",
    "    def update(self, special_word: str) -> None:\n",
    "        curr_state = 0\n",
    "\n",
    "        for idx, special_char in enumerate(special_word):\n",
    "            if curr_state not in self.fsm:\n",
    "                self.fsm[curr_state] = []\n",
    "\n",
    "            state2tokens = [items[0] for items in self.fsm[curr_state]]\n",
    "\n",
    "            if special_char not in state2tokens:\n",
    "                if idx == len(special_word) - 1:\n",
    "                    self.fsm[curr_state].append([special_char, self.end_state])\n",
    "                else:\n",
    "                    self.fsm[curr_state].append([special_char, self.next_state])\n",
    "                    curr_state = self.next_state\n",
    "                    self.next_state += 1\n",
    "            else:\n",
    "                for fsm_idx in range(len(self.fsm[curr_state])):\n",
    "                    if special_char == self.fsm[curr_state][fsm_idx][0] and idx == len(special_word) - 1:\n",
    "                        self.fsm[curr_state][fsm_idx][1] = self.end_state\n",
    "                        break\n",
    "                    elif special_char == self.fsm[curr_state][fsm_idx][0]:\n",
    "                        curr_state = self.fsm[curr_state][fsm_idx][1]\n",
    "                        break\n",
    "\n",
    "    def update_group(self, special_words: List[str]) -> None:\n",
    "        for special_word in special_words:\n",
    "            self.update(special_word=special_word)\n",
    "\n",
    "    def get_fsm_data(self) -> Dict[str, List[Tuple[int, int]]]:\n",
    "        return self.fsm\n",
    "    \n",
    "    def detect(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if the current token leads to a sensitive sequence.\n",
    "        Updates the current state and returns True if it reaches the end state.\n",
    "        \"\"\"\n",
    "        for char in token:\n",
    "            for transition in self.fsm[self.curr_state]:\n",
    "                if transition[0] == char:\n",
    "                    self.curr_state = transition[1]\n",
    "\n",
    "                    # If the current state reaches the end state\n",
    "                    return self.curr_state == self.end_state\n",
    "\n",
    "\n",
    "        if self.curr_state in self.fsm:\n",
    "            for transition in self.fsm[self.curr_state]:\n",
    "                if transition[0] == token:\n",
    "                    self.curr_state = transition[1]\n",
    "\n",
    "                    # If the current state reaches the end state\n",
    "                    return self.curr_state == self.end_state\n",
    "        \n",
    "        # If the token does not match, reset the current state\n",
    "        self.curr_state = 0\n",
    "        return False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
