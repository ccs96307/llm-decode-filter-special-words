{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"./models/google--gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.bfloat16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_words = [\"talk\", \"listen\", \"fuck you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered First Candidates: [['li'], ['fuck'], ['tal'], ['l'], ['lis'], ['ta'], ['liste'], ['t'], ['talk'], ['listen'], ['f'], ['list'], ['fu']]\n",
      "Filtered First IDs: [[515], [34024], [3559], [235257], [15063], [516], [44003], [235251], [33085], [18998], [235266], [1701], [12819]]\n",
      "Final Routes: [['talk'], ['listen'], ['li', 'sten'], ['l', 'isten'], ['tal', 'k'], ['liste', 'n'], ['list', 'en'], ['lis', 'ten'], ['ta', 'lk'], ['t', 'alk'], ['l', 'i', 'sten'], ['ta', 'l', 'k'], ['t', 'al', 'k'], ['l', 'iste', 'n'], ['lis', 'te', 'n'], ['li', 'ste', 'n'], ['list', 'e', 'n'], ['li', 'st', 'en'], ['l', 'ist', 'en'], ['lis', 't', 'en'], ['l', 'is', 'ten'], ['li', 's', 'ten'], ['t', 'a', 'lk'], ['t', 'a', 'l', 'k'], ['l', 'is', 'te', 'n'], ['li', 's', 'te', 'n'], ['l', 'i', 'ste', 'n'], ['li', 'st', 'e', 'n'], ['l', 'ist', 'e', 'n'], ['lis', 't', 'e', 'n'], ['l', 'i', 'st', 'en'], ['l', 'is', 't', 'en'], ['li', 's', 't', 'en'], ['l', 'i', 's', 'ten'], ['l', 'i', 's', 'te', 'n'], ['l', 'i', 'st', 'e', 'n'], ['l', 'is', 't', 'e', 'n'], ['li', 's', 't', 'e', 'n'], ['l', 'i', 's', 't', 'en'], ['l', 'i', 's', 't', 'e', 'n']]\n",
      "Final ID Routes: [[33085], [18998], [515, 5547], [235257, 17071], [3559, 235273], [44003, 235254], [1701, 479], [15063, 965], [516, 26159], [235251, 2071], [235257, 235252, 5547], [516, 235257, 235273], [235251, 492, 235273], [235257, 3671, 235254], [15063, 488, 235254], [515, 2855, 235254], [1701, 235249, 235254], [515, 490, 479], [235257, 694, 479], [15063, 235251, 479], [235257, 502, 965], [515, 235256, 965], [235251, 235250, 26159], [235251, 235250, 235257, 235273], [235257, 502, 488, 235254], [515, 235256, 488, 235254], [235257, 235252, 2855, 235254], [515, 490, 235249, 235254], [235257, 694, 235249, 235254], [15063, 235251, 235249, 235254], [235257, 235252, 490, 479], [235257, 502, 235251, 479], [515, 235256, 235251, 479], [235257, 235252, 235256, 965], [235257, 235252, 235256, 488, 235254], [235257, 235252, 490, 235249, 235254], [235257, 502, 235251, 235249, 235254], [515, 235256, 235251, 235249, 235254], [235257, 235252, 235256, 235251, 479], [235257, 235252, 235256, 235251, 235249, 235254]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "new_token_candidates = []\n",
    "new_token_id_candidates = []\n",
    "\n",
    "# Define a pattern to filter out unwanted excessive whitespace tokens (e.g., multiple tabs or newlines)\n",
    "invalid_token_pattern = re.compile(r'^[\\t\\n\\r\\f\\v\\s]*$')  # Matches tokens that are made of 3 or more whitespace characters\n",
    "\n",
    "# Initial matching of tokens that start with banned word prefixes\n",
    "for token, token_id in tokenizer.vocab.items():\n",
    "    token_stripped = token.strip()\n",
    "\n",
    "    # Filter out tokens that consist of excessive whitespace or control characters\n",
    "    if invalid_token_pattern.match(token_stripped):\n",
    "        continue\n",
    "\n",
    "    for banned_word in banned_words:\n",
    "        if banned_word.startswith(token_stripped):\n",
    "            new_token_candidates.append([token])  # Store as a list for consistency\n",
    "            new_token_id_candidates.append([token_id])\n",
    "\n",
    "print(\"Filtered First Candidates:\", new_token_candidates)\n",
    "print(\"Filtered First IDs:\", new_token_id_candidates)\n",
    "\n",
    "# Initialize final routes for fully matched banned words\n",
    "final_routes = []\n",
    "final_id_routes = []\n",
    "for new_token, new_token_id in zip(new_token_candidates, new_token_id_candidates):\n",
    "    if \"\".join(new_token).strip() in banned_words:\n",
    "        final_routes.append(new_token)\n",
    "        final_id_routes.append(new_token_id)\n",
    "\n",
    "# Iteratively expand candidates using BFS-like approach\n",
    "while new_token_candidates:\n",
    "    curr_token_candidates = new_token_candidates\n",
    "    curr_token_id_candidates = new_token_id_candidates\n",
    "    new_token_candidates = []\n",
    "    new_token_id_candidates = []\n",
    "\n",
    "    # Iterate over vocabulary and expand each candidate\n",
    "    for token, token_id in tokenizer.vocab.items():\n",
    "        token_stripped = token.strip()\n",
    "\n",
    "        # Skip tokens that consist of excessive whitespace or control characters\n",
    "        if invalid_token_pattern.match(token_stripped):\n",
    "            continue\n",
    "\n",
    "        for candidate_token, candidate_token_id in zip(curr_token_candidates, curr_token_id_candidates):\n",
    "            curr_token = candidate_token + [token]\n",
    "            curr_token_ids = candidate_token_id + [token_id]\n",
    "\n",
    "            curr_token_str = \"\".join(curr_token).strip()\n",
    "\n",
    "            # Check if the current token combination matches or is a prefix of any banned word\n",
    "            for banned_word in banned_words:\n",
    "                if curr_token_str == banned_word:\n",
    "                    # Full match found, add to final routes\n",
    "                    final_routes.append(curr_token)\n",
    "                    final_id_routes.append(curr_token_ids)\n",
    "                elif banned_word.startswith(curr_token_str):\n",
    "                    # Partial match, keep expanding this candidate\n",
    "                    new_token_candidates.append(curr_token)\n",
    "                    new_token_id_candidates.append(curr_token_ids)\n",
    "\n",
    "print(\"Final Routes:\", final_routes)\n",
    "print(\"Final ID Routes:\", final_id_routes)\n",
    "\n",
    "# Optimizations:\n",
    "# 1. Added regex filtering (`invalid_token_pattern`) to filter out tokens made of excessive whitespace or control characters.\n",
    "# 2. Removed unnecessary deep copy (`copy.deepcopy`), instead directly reassign lists which is more efficient.\n",
    "# 3. Combined stripping and joining operations to reduce redundant code.\n",
    "# 4. Improved readability by adding meaningful comments for each logical section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talk'] [33085]\n",
      "['listen'] [18998]\n",
      "['li', 'sten'] [515, 5547]\n",
      "['l', 'isten'] [235257, 17071]\n",
      "['tal', 'k'] [3559, 235273]\n",
      "['liste', 'n'] [44003, 235254]\n",
      "['list', 'en'] [1701, 479]\n",
      "['lis', 'ten'] [15063, 965]\n",
      "['ta', 'lk'] [516, 26159]\n",
      "['t', 'alk'] [235251, 2071]\n",
      "['l', 'i', 'sten'] [235257, 235252, 5547]\n",
      "['ta', 'l', 'k'] [516, 235257, 235273]\n",
      "['t', 'al', 'k'] [235251, 492, 235273]\n",
      "['l', 'iste', 'n'] [235257, 3671, 235254]\n",
      "['lis', 'te', 'n'] [15063, 488, 235254]\n",
      "['li', 'ste', 'n'] [515, 2855, 235254]\n",
      "['list', 'e', 'n'] [1701, 235249, 235254]\n",
      "['li', 'st', 'en'] [515, 490, 479]\n",
      "['l', 'ist', 'en'] [235257, 694, 479]\n",
      "['lis', 't', 'en'] [15063, 235251, 479]\n",
      "['l', 'is', 'ten'] [235257, 502, 965]\n",
      "['li', 's', 'ten'] [515, 235256, 965]\n",
      "['t', 'a', 'lk'] [235251, 235250, 26159]\n",
      "['t', 'a', 'l', 'k'] [235251, 235250, 235257, 235273]\n",
      "['l', 'is', 'te', 'n'] [235257, 502, 488, 235254]\n",
      "['li', 's', 'te', 'n'] [515, 235256, 488, 235254]\n",
      "['l', 'i', 'ste', 'n'] [235257, 235252, 2855, 235254]\n",
      "['li', 'st', 'e', 'n'] [515, 490, 235249, 235254]\n",
      "['l', 'ist', 'e', 'n'] [235257, 694, 235249, 235254]\n",
      "['lis', 't', 'e', 'n'] [15063, 235251, 235249, 235254]\n",
      "['l', 'i', 'st', 'en'] [235257, 235252, 490, 479]\n",
      "['l', 'is', 't', 'en'] [235257, 502, 235251, 479]\n",
      "['li', 's', 't', 'en'] [515, 235256, 235251, 479]\n",
      "['l', 'i', 's', 'ten'] [235257, 235252, 235256, 965]\n",
      "['l', 'i', 's', 'te', 'n'] [235257, 235252, 235256, 488, 235254]\n",
      "['l', 'i', 'st', 'e', 'n'] [235257, 235252, 490, 235249, 235254]\n",
      "['l', 'is', 't', 'e', 'n'] [235257, 502, 235251, 235249, 235254]\n",
      "['li', 's', 't', 'e', 'n'] [515, 235256, 235251, 235249, 235254]\n",
      "['l', 'i', 's', 't', 'en'] [235257, 235252, 235256, 235251, 479]\n",
      "['l', 'i', 's', 't', 'e', 'n'] [235257, 235252, 235256, 235251, 235249, 235254]\n"
     ]
    }
   ],
   "source": [
    "for items, ids in zip(final_routes, final_id_routes):\n",
    "    print(items, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Can we talk?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(input_ids: torch.Tensor, max_length: int = 50) -> str:\n",
    "    for _ in range(max_length):\n",
    "        # Generate new tokens\n",
    "        outputs = model(input_ids.to(device), return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        new_generated_token_id = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if new_generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat((input_ids, new_generated_token_id.unsqueeze(0)), dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to listen and help in any way I can. \n",
      "\n",
      "What's on your mind? \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate(input_ids=inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSMProcessor:\n",
    "    def __init__(self, special_token_ids_list: List[List[int]], end_state: int = -1) -> None:\n",
    "        self.end_state = end_state\n",
    "        self.next_state = 1\n",
    "        self.curr_state = 0\n",
    "        self.fsm = {}\n",
    "        self.special_words = []\n",
    "\n",
    "        # Track partial matches\n",
    "        self.partial_match_state = None\n",
    "        self.partial_tokens = []\n",
    "\n",
    "        self.update_group(special_token_ids_list)\n",
    "\n",
    "    def update(self, special_token_ids: List[int]) -> None:\n",
    "        curr_state = 0\n",
    "\n",
    "        for idx, special_token_id in enumerate(special_token_ids):\n",
    "            if curr_state not in self.fsm:\n",
    "                self.fsm[curr_state] = []\n",
    "\n",
    "            state2id = [items[0] for items in self.fsm[curr_state]]\n",
    "            if special_token_id not in state2id:\n",
    "                if idx == len(special_token_ids) - 1:\n",
    "                    self.fsm[curr_state].append([special_token_id, self.end_state])\n",
    "                else:\n",
    "                    self.fsm[curr_state].append([special_token_id, self.next_state])\n",
    "                    curr_state = self.next_state\n",
    "                    self.next_state += 1\n",
    "            else:\n",
    "                for fsm_idx in range(len(self.fsm[curr_state])):\n",
    "                    if special_token_id == self.fsm[curr_state][fsm_idx][0] and idx == len(special_token_ids) - 1:\n",
    "                        self.fsm[curr_state][fsm_idx][1] = self.end_state\n",
    "                        break\n",
    "                    elif special_token_id == self.fsm[curr_state][fsm_idx][0]:\n",
    "                        curr_state = self.fsm[curr_state][fsm_idx][1]\n",
    "                        break\n",
    "\n",
    "    def update_group(self, special_token_ids_list: List[List[int]]) -> None:\n",
    "        for special_token_ids in special_token_ids_list:\n",
    "            self.update(special_token_ids=special_token_ids)\n",
    "\n",
    "    def get_fsm_data(self) -> Dict[str, List[Tuple[int, int]]]:\n",
    "        return self.fsm\n",
    "    \n",
    "    def detect(self, token: int) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if the current token leads to a sensitive sequence.\n",
    "        Updates the current state and returns True if it reaches the end state.\n",
    "        \"\"\"\n",
    "        if self.curr_state in self.fsm:\n",
    "            for transition in self.fsm[self.curr_state]:\n",
    "                if transition[0] == token:\n",
    "                    self.curr_state = transition[1]\n",
    "\n",
    "                    # If the current state reaches the end state\n",
    "                    return self.curr_state == self.end_state\n",
    "        \n",
    "        # If the token does not match, reset the current state\n",
    "        self.curr_state = 0\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[33085, -1],\n",
       "  [18998, -1],\n",
       "  [515, 1],\n",
       "  [235257, 2],\n",
       "  [3559, 3],\n",
       "  [44003, 4],\n",
       "  [1701, 5],\n",
       "  [15063, 6],\n",
       "  [516, 7],\n",
       "  [235251, 8]],\n",
       " 1: [[5547, -1], [2855, 14], [490, 16], [235256, 20]],\n",
       " 2: [[17071, -1], [235252, 9], [3671, 12], [694, 17], [502, 19]],\n",
       " 3: [[235273, -1]],\n",
       " 4: [[235254, -1]],\n",
       " 5: [[479, -1], [235249, 15]],\n",
       " 6: [[965, -1], [488, 13], [235251, 18]],\n",
       " 7: [[26159, -1], [235257, 10]],\n",
       " 8: [[2071, -1], [492, 11], [235250, 21]],\n",
       " 9: [[5547, -1], [2855, 25], [490, 29], [235256, 32]],\n",
       " 10: [[235273, -1]],\n",
       " 11: [[235273, -1]],\n",
       " 12: [[235254, -1]],\n",
       " 13: [[235254, -1]],\n",
       " 14: [[235254, -1]],\n",
       " 15: [[235254, -1]],\n",
       " 16: [[479, -1], [235249, 26]],\n",
       " 17: [[479, -1], [235249, 27]],\n",
       " 18: [[479, -1], [235249, 28]],\n",
       " 19: [[965, -1], [488, 23], [235251, 30]],\n",
       " 20: [[965, -1], [488, 24], [235251, 31]],\n",
       " 21: [[26159, -1], [235257, 22]],\n",
       " 22: [[235273, -1]],\n",
       " 23: [[235254, -1]],\n",
       " 24: [[235254, -1]],\n",
       " 25: [[235254, -1]],\n",
       " 26: [[235254, -1]],\n",
       " 27: [[235254, -1]],\n",
       " 28: [[235254, -1]],\n",
       " 29: [[479, -1], [235249, 34]],\n",
       " 30: [[479, -1], [235249, 35]],\n",
       " 31: [[479, -1], [235249, 36]],\n",
       " 32: [[965, -1], [488, 33], [235251, 37]],\n",
       " 33: [[235254, -1]],\n",
       " 34: [[235254, -1]],\n",
       " 35: [[235254, -1]],\n",
       " 36: [[235254, -1]],\n",
       " 37: [[479, -1], [235249, 38]],\n",
       " 38: [[235254, -1]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm_processor = FSMProcessor(special_token_ids_list=final_id_routes)\n",
    "fsm_processor.get_fsm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate_with_fsm_filter(\n",
    "    input_ids: torch.Tensor,\n",
    "    fsm_processor: FSMProcessor,\n",
    "    max_length: int = 20,\n",
    ") -> str:\n",
    "    # Historical mask list used to record masked tokens at each decoding step\n",
    "    masked_tokens_history = {}\n",
    "    past_key_values = None\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_length:\n",
    "        steps += 1\n",
    "\n",
    "        # Generate new token with kv cache\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, return_dict=True, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Update kv cache\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Check if there are already masked tokens at the current step\n",
    "        if steps in masked_tokens_history:\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "        else:\n",
    "            masked_tokens_history[steps] = set()\n",
    "\n",
    "        # Decode the generated token\n",
    "        generated_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        combined_ids = torch.cat((input_ids, torch.tensor([[generated_token_id]], device=input_ids.device)), dim=-1)\n",
    "\n",
    "        # Check FSM for sensitive sequences\n",
    "        if fsm_processor.detect(generated_token_id):\n",
    "            # Detected a sensitive sequence, initiate rollback\n",
    "            rollback_length = fsm_processor.partial_match_state + 1 if fsm_processor.partial_match_state is not None else 1\n",
    "            steps = steps - rollback_length + 1\n",
    "            rollbacks_ids = combined_ids[:, :-rollback_length]\n",
    "            input_ids = rollbacks_ids\n",
    "            print(f\"Rollback detected. Rolling back from step {steps + rollback_length} to step {steps}\")\n",
    "\n",
    "            # Reset FSM state\n",
    "            fsm_processor.curr_state = 0\n",
    "            fsm_processor.partial_match_state = None\n",
    "\n",
    "            # Reset past_key_values when rolling back\n",
    "            past_key_values = None\n",
    "\n",
    "            # Recalculate logits based on rolled-back sequence\n",
    "            outputs = model(input_ids, return_dict=True, use_cache=True)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # Mask the first token of the sensitive sequence\n",
    "            first_token_id = generated_token_id\n",
    "            print(f\"Masking token id: {first_token_id}, Masking token: {tokenizer.decode(first_token_id)}\")\n",
    "\n",
    "            # Update the historical mask list to record the token at this step\n",
    "            masked_tokens_history[steps].add(first_token_id)\n",
    "\n",
    "            for masked_token_id in masked_tokens_history[steps]:\n",
    "                logits[:, masked_token_id] = -float(\"inf\")\n",
    "\n",
    "            # Generate the token again after masking\n",
    "            generated_token_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        # Update input_ids with the generated token\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([[generated_token_id]], device=input_ids.device)), dim=1)\n",
    "\n",
    "        print(f\"Step {steps}: ID: {generated_token_id} Generated token: {tokenizer.decode(generated_token_id)}\")\n",
    "\n",
    "        if generated_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 2: ID: 235285 Generated token: I\n",
      "Step 3: ID: 235303 Generated token: '\n",
      "Step 4: ID: 235262 Generated token: m\n",
      "Step 5: ID: 1517 Generated token:  here\n",
      "Step 6: ID: 577 Generated token:  to\n",
      "Step 7: ID: 10724 Generated token:  listen\n",
      "Step 8: ID: 578 Generated token:  and\n",
      "Step 9: ID: 1707 Generated token:  help\n",
      "Step 10: ID: 575 Generated token:  in\n",
      "Step 11: ID: 1089 Generated token:  any\n",
      "Step 12: ID: 1703 Generated token:  way\n",
      "Step 13: ID: 590 Generated token:  I\n",
      "Step 14: ID: 798 Generated token:  can\n",
      "Step 15: ID: 235265 Generated token: .\n",
      "Step 16: ID: 235248 Generated token:  \n",
      "Step 17: ID: 109 Generated token: \n",
      "\n",
      "\n",
      "Step 18: ID: 1841 Generated token: What\n",
      "Step 19: ID: 235303 Generated token: '\n",
      "Step 20: ID: 235256 Generated token: s\n",
      "Step 21: ID: 611 Generated token:  on\n",
      "Step 22: ID: 861 Generated token:  your\n",
      "Step 23: ID: 3403 Generated token:  mind\n",
      "Step 24: ID: 235336 Generated token: ?\n",
      "Step 25: ID: 235248 Generated token:  \n",
      "Step 26: ID: 108 Generated token: \n",
      "\n",
      "Step 27: ID: 107 Generated token: <end_of_turn>\n",
      "Step 28: ID: 1 Generated token: <eos>\n",
      "<bos>Can we talk?\n",
      "\n",
      "I'm here to listen and help in any way I can. \n",
      "\n",
      "What's on your mind? \n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(custom_generate_with_fsm_filter(\n",
    "    input_ids=inputs.input_ids,\n",
    "    fsm_processor=fsm_processor,\n",
    "    max_length=50,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
